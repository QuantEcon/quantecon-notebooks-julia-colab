{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='numerical-linear-algebra'></a>\n",
    "<div id=\"qe-notebook-header\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra and Factorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Numerical Linear Algebra and Factorizations](#Numerical-Linear-Algebra-and-Factorizations)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Factorizations](#Factorizations)  \n",
    "  - [Continuous Time Markov Chains (CTMC)](#Continuous-Time-Markov-Chains-%28CTMC%29)  \n",
    "  - [Banded Matrices](#Banded-Matrices)  \n",
    "  - [Implementation Details and Performance](#Implementation-Details-and-Performance)  \n",
    "  - [Exercises](#Exercises)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You cannot learn too much linear algebra. – Benedict Gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture, we examine the structure of matrices and linear operators (e.g., dense, sparse, symmetric, tridiagonal, banded) and\n",
    "discuss how the structure can be exploited to radically increase the performance of solving large problems.\n",
    "\n",
    "We build on applications discussed in previous lectures: [linear algebra](linear_algebra.html), [orthogonal projections](orth_proj.html), and [Markov chains](finite_markov.html).\n",
    "\n",
    "The methods in this section are called direct methods, and they are qualitatively similar to performing Gaussian elimination to factor matrices and solve systems of equations.  In [iterative methods and sparsity](iterative_methods_sparsity.html) we examine a different approach, using iterative algorithms, where we can think of more general linear operators.\n",
    "\n",
    "The list of specialized packages for these tasks is enormous and growing, but some of the important organizations to\n",
    "look at are [JuliaMatrices](https://github.com/JuliaMatrices) , [JuliaSparse](https://github.com/JuliaSparse), and [JuliaMath](https://github.com/JuliaMath)\n",
    "\n",
    "*NOTE*: As this section uses advanced Julia techniques, you may wish to review multiple-dispatch and generic programming in  introduction to types, and consider further study on [generic programming](../more_julia/generic_programming.html).\n",
    "\n",
    "The theme of this lecture, and numerical linear algebra in general, comes down to three principles:\n",
    "\n",
    "1. **Identify structure** (e.g. [symmetric, sparse, diagonal, etc.](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#Special-matrices-1)) of matrices in order to use **specialized algorithms.**  \n",
    "1. **Do not lose structure** by applying the wrong numerical linear algebra operations at the wrong times (e.g. sparse matrix becoming dense)  \n",
    "1. Understand the **computational complexity** of each algorithm, given the structure of the inputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide-output": true
   },
   "outputs": [],
   "source": [
    "using InstantiateFromURL\n",
    "# optionally add arguments to force installation: instantiate = true, precompile = true\n",
    "github_project(\"QuantEcon/quantecon-notebooks-julia\", version = \"0.7.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide-output": true
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics, BenchmarkTools, SparseArrays, Random\n",
    "Random.seed!(42);  # seed random numbers for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "Ask yourself whether the following is a **computationally expensive** operation as the matrix **size increases**\n",
    "\n",
    "- Multiplying two matrices?  \n",
    "  \n",
    "  - *Answer*: It depends.  Multiplying 2 diagonal matrices is trivial.  \n",
    "  \n",
    "- Solving a linear system of equations?  \n",
    "  \n",
    "  - *Answer*: It depends.  If the matrix is the identity, the solution is the vector itself.  \n",
    "  \n",
    "- Finding the eigenvalues of a matrix?  \n",
    "  \n",
    "  - *Answer*: It depends.  The eigenvalues of a triangular matrix are the diagonal.  \n",
    "  \n",
    "\n",
    "\n",
    "As the goal of this section is to move towards numerical methods with large systems, we need to understand how well algorithms scale with the size of matrices/vectors/etc.  This is known as [computational complexity](https://en.wikipedia.org/wiki/Computational_complexity).  As we saw in the answer to the questions above, the algorithm - and hence the computational complexity - changes based on matrix structure.\n",
    "\n",
    "While this notion of complexity can work at various levels such as the number of [significant digits](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions) for basic mathematical operations, the amount of memory and storage required, or the amount of time - we will typically focus on the time-complexity.\n",
    "\n",
    "For time-complexity, the size $ N $ is usually the dimensionality of the problem, although occasionally the key will be the number of non-zeros in the matrix or width of bands.  For our applications, time-complexity is best thought of as the number of floating point operations (e.g. addition, multiplication, etc.) required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "\n",
    "Complexity of algorithms is typically written in [Big O](https://en.wikipedia.org/wiki/Big_O_notation) notation which provides bounds on the scaling of the computational complexity with respect to the size of the inputs.\n",
    "\n",
    "Formally, if the number of operations required for a problem size $ N $ is $ f(N) $, we  can write this as $ f(N) = O(g(N)) $ for some $ g(N) $ - typically a polynomial.\n",
    "\n",
    "The interpretation is that there exists some constants $ M $ and $ N_0 $ such that\n",
    "\n",
    "$$\n",
    "f(N) \\leq M g(N), \\text{ for } N > N_0\n",
    "$$\n",
    "\n",
    "For example, the complexity of finding an LU Decomposition of a dense matrix is $ O(N^3) $ which should be read as there being a constant where\n",
    "eventually the number of floating point operations required to decompose a matrix of size $ N\\times N $ grows cubically.\n",
    "\n",
    "Keep in mind that these are asymptotic results intended for understanding the scaling of the problem, and the constant can matter for a given\n",
    "fixed size.\n",
    "\n",
    "For example, the number of operations required for an [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition#Algorithms) of a dense $ N \\times N $ matrix is $ f(N) = \\frac{2}{3} N^3 $, ignoring the $ N^2 $ and lower terms.  Other methods of solving a linear system may have different constants of proportionality, even if they have the same scaling $ O(N^3) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of Computational Complexity\n",
    "\n",
    "You will sometimes need to think through how [combining algorithms](https://en.wikipedia.org/wiki/Big_O_notation#Properties) changes complexity.  For example, if you use\n",
    "\n",
    "1. An $ O(N^3) $ operation $ P $ times, then it simply changes the constant. The complexity remains $ O(N^3) $.  \n",
    "1. One $ O(N^3) $ operation and another $ O(N^2) $ one, then you take the max.  The complexity remains $ O(N^3) $.  \n",
    "1. A repetition of a $ O(N) $ operation that itself uses an $ O(N) $ one, you take the product.  The complexity becomes $ O(N^2) $.  \n",
    "\n",
    "\n",
    "With this, we have an important word of caution: dense matrix-multiplication is an [expensive operation](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra) for unstructured matrices.  The naive version is $ O(N^3) $ while the fastest known algorithms (e.g Coppersmith-Winograd) are roughly $ O(N^{2.37}) $.  In practice, it is reasonable to crudely approximate with $ O(N^3) $ when doing an analysis, in part since the higher constant factors of the better scaling algorithms dominate the better complexity until matrices become very large.\n",
    "\n",
    "Of course, modern libraries use highly tuned and numerically stable [algorithms](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm) to multiply matrices and exploit the computer architecture, memory cache, etc., but this simply lowers the constant of proportionality and they remain roughly approximated by  $ O(N^3) $.\n",
    "\n",
    "A consequence is that, since many algorithms require matrix-matrix multiplication, it is often not possible to go below that order without further matrix structure.\n",
    "\n",
    "That is, changing the constant of proportionality for a given size can help, but in order to achieve better scaling you need to identify matrix structure (e.g. tridigonal, sparse, etc.) and ensure your operations do not lose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losing Structure\n",
    "\n",
    "As a first example of a structured matrix, consider a [sparse arrays](https://docs.julialang.org/en/v1/stdlib/SparseArrays/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnz(A) = 47\n",
      "nnz(invA) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "A = sprand(10, 10, 0.45)  # random sparse 10x10, 45 percent filled with non-zeros\n",
    "\n",
    "@show nnz(A)  # counts the number of non-zeros\n",
    "invA = sparse(inv(Array(A)))  # julia won't invert sparse so convert to dense with Array.\n",
    "@show nnz(invA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This increase from less than 50 to 100 percent dense demonstrates that significant sparsity can be lost when computing an inverse.\n",
    "\n",
    "The results can be even more extreme.  Consider a tridiagonal matrix of size $ N \\times N $\n",
    "that might come out of a Markov Chain or a discretization of a diffusion process,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " 0.8  0.2   ⋅    ⋅    ⋅ \n",
       " 0.1  0.8  0.1   ⋅    ⋅ \n",
       "  ⋅   0.1  0.8  0.1   ⋅ \n",
       "  ⋅    ⋅   0.1  0.8  0.1\n",
       "  ⋅    ⋅    ⋅   0.2  0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of non-zeros here is approximately $ 3 N $, linear, which scales well for huge matrices into the millions or billions\n",
    "\n",
    "But consider the inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.29099      -0.327957     0.0416667  -0.00537634   0.000672043\n",
       " -0.163978      1.31183     -0.166667    0.0215054   -0.00268817\n",
       "  0.0208333    -0.166667     1.29167    -0.166667     0.0208333\n",
       " -0.00268817    0.0215054   -0.166667    1.31183     -0.163978\n",
       "  0.000672043  -0.00537634   0.0416667  -0.327957     1.29099"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the matrix is fully dense and has $ N^2 $ non-zeros.\n",
    "\n",
    "This also applies to the $ A' A $ operation when forming the normal equations of linear-least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnz(A) / 20 ^ 2 = 0.2825\n",
      "nnz(A' * A) / 21 ^ 2 = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800453514739229\n"
     ]
    }
   ],
   "source": [
    "A = sprand(20, 21, 0.3)\n",
    "@show nnz(A)/20^2\n",
    "@show nnz(A'*A)/21^2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a 30 percent dense matrix becomes almost full dense after the product is taken.\n",
    "\n",
    "*Sparsity/Structure is not just for storage*:  Matrix size can sometimes become important (e.g. a 1 million by 1 million tridiagonal matrix needs to store 3 million numbers (i.e, about 6MB of memory), where a dense one requires 1 trillion (i.e., about 1TB of memory).\n",
    "\n",
    "But, as we will see, the main purpose of considering sparsity and matrix structure is that it enables specialized algorithms which typically\n",
    "have a lower-computational order than unstructured dense, or even unstructured sparse operations.\n",
    "\n",
    "First, create a convenient functions for benchmarking linear solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark_solve (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "function benchmark_solve(A, b)\n",
    "    println(\"A\\\\b for typeof(A) = $(string(typeof(A)))\")\n",
    "    @btime $A \\ $b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, take away structure to see the impact on performance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\\b for typeof(A) = Tridiagonal{Float64,Array{Float64,1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  22.109 μs (9 allocations: 47.75 KiB)\n",
      "A\\b for typeof(A) = SparseMatrixCSC{Float64,Int64}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  602.597 μs (69 allocations: 1.06 MiB)\n",
      "A\\b for typeof(A) = Array{Float64,2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  135.262 ms (5 allocations: 7.65 MiB)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "b = rand(N)\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])\n",
    "A_sparse = sparse(A)  # sparse but losing tridiagonal structure\n",
    "A_dense = Array(A)    # dropping the sparsity structure, dense 1000x1000\n",
    "\n",
    "# benchmark solution to system A x = b\n",
    "benchmark_solve(A, b)\n",
    "benchmark_solve(A_sparse, b)\n",
    "benchmark_solve(A_dense, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows what is at stake:  using a structured tridiagonal matrix may be 10-20x faster than using a sparse matrix which is 100x faster than\n",
    "using a dense matrix.\n",
    "\n",
    "In fact, the difference becomes more extreme as the matrices grow.  Solving a tridiagonal system is $ O(N) $ while that of a dense matrix without any structure is $ O(N^3) $.  The complexity of a sparse solution is more complicated, and scales in part by the `nnz(N)`, i.e. the number of nonzeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "While we write matrix multiplications in our algebra with abundance, in practice the computational operation scales very poorly without any matrix structure.\n",
    "\n",
    "Matrix multiplication is so important to modern computers that the constant of scaling is small using proper packages, but the order is still roughly $ O(N^3) $ in practice (although smaller in theory, as discussed above).\n",
    "\n",
    "Sparse matrix multiplication, on the other hand, is $ O(N M_A M_B) $ where $ M_A $ are the number of nonzeros per row of $ A $ and $ M_B $ are the number of non-zeros per column of $ B $.\n",
    "\n",
    "By the rules of computational order, that means any algorithm requiring a matrix multiplication of dense matrices requires at least $ O(N^3) $ operation.\n",
    "\n",
    "The other important question is what is the structure of the resulting matrix.  For example, multiplying an upper triangular by a lower triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 UpperTriangular{Float64,Array{Float64,2}}:\n",
       " 0.299976  0.176934  0.0608682  0.20465   0.409653\n",
       "  ⋅        0.523923  0.127154   0.512531  0.235328\n",
       "  ⋅         ⋅        0.600588   0.682868  0.330638\n",
       "  ⋅         ⋅         ⋅         0.345419  0.0312986\n",
       "  ⋅         ⋅         ⋅          ⋅        0.471043"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "U = UpperTriangular(rand(N,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Adjoint{Float64,UpperTriangular{Float64,Array{Float64,2}}}:\n",
       " 0.299976   0.0       0.0       0.0        0.0\n",
       " 0.176934   0.523923  0.0       0.0        0.0\n",
       " 0.0608682  0.127154  0.600588  0.0        0.0\n",
       " 0.20465    0.512531  0.682868  0.345419   0.0\n",
       " 0.409653   0.235328  0.330638  0.0312986  0.471043"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = U'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the product is fully dense (e.g. think of a cholesky multiplied by itself to produce a covariance matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 0.0899855  0.0530758  0.018259   0.0613901  0.122886\n",
       " 0.0530758  0.305801   0.0773883  0.304736   0.195775\n",
       " 0.018259   0.0773883  0.380579   0.487749   0.253435\n",
       " 0.0613901  0.304736   0.487749   0.890193   0.441042\n",
       " 0.122886   0.195775   0.253435   0.441042   0.555378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L * U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, a tridiagonal times a diagonal is still a tridiagonal - and can use specialized $ O(N) $ algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " 0.0156225  0.00390564   ⋅           ⋅          ⋅ \n",
       " 0.0436677  0.349342    0.0436677    ⋅          ⋅ \n",
       "  ⋅         0.0213158   0.170526    0.0213158   ⋅ \n",
       "  ⋅          ⋅          0.00790566  0.0632453  0.00790566\n",
       "  ⋅          ⋅           ⋅          0.19686    0.787442"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])\n",
    "D = Diagonal(rand(N))\n",
    "D * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorizations\n",
    "\n",
    "When you tell a numerical analyst you are solving a linear system using direct methods, their first question is “which factorization?”.\n",
    "\n",
    "Just as you can factorize a number (e.g. $ 6 = 3 \\times 2 $) you can factorize a matrix as the product of other, more\n",
    "convenient matrices (e.g. $ A = L U $ or $ A = Q R $ where $ L, U, Q, $ and $ R $ have properties such as being triangular or [orthogonal](https://en.wikipedia.org/wiki/Orthogonal_matrix), etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverting Matrices\n",
    "\n",
    "On paper, since the [Invertible Matrix Theorem](https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem) tells us a unique solution is\n",
    "equivalent to $ A $ being invertible, we often write the solution to $ A x = b $ as\n",
    "\n",
    "$$\n",
    "x = A^{-1} b\n",
    "$$\n",
    "\n",
    "What if we do not (directly) use a factorization?\n",
    "\n",
    "Take a simple linear system of a dense matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.5682240701809245\n",
       " 0.40245385575255055\n",
       " 0.1825995192132288\n",
       " 0.06160128039631019"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "A = rand(N,N)\n",
    "b = rand(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On paper, we try to solve the system $ A x = b $ by inverting the matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " -0.033906984040767744\n",
       "  0.7988200873225004\n",
       "  0.9963711951331815\n",
       " -0.9276352098500461"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = inv(A) * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see throughout, inverting matrices should be used for theory, not for code.  The classic advice that you should [never invert a matrix](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix) may be [slightly exaggerated](https://arxiv.org/abs/1201.6035), but is generally good advice.\n",
    "\n",
    "Solving a system by inverting a matrix is always a little slower, potentially less accurate, and will sometimes lose crucial sparsity compared to using factorizations.  Moreover, the methods used by libraries to invert matrices are frequently the same factorizations used for computing a system of equations.\n",
    "\n",
    "Even if you need to solve a system with the same matrix multiple times, you are better off factoring the matrix and using the solver rather than calculating an inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.462 ms (68 allocations: 205.28 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.025 ms (96 allocations: 155.59 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.003 ms (6 allocations: 102.63 KiB)\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "A = rand(N,N)\n",
    "M = 30\n",
    "B = rand(N,M)\n",
    "function solve_inverting(A, B)\n",
    "    A_inv = inv(A)\n",
    "    X = similar(B)\n",
    "    for i in 1:size(B,2)\n",
    "        X[:,i] = A_inv * B[:,i]\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "function solve_factoring(A, B)\n",
    "    X = similar(B)\n",
    "    A = factorize(A)\n",
    "    for i in 1:size(B,2)\n",
    "        X[:,i] = A \\ B[:,i]\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "@btime solve_inverting($A, $B)\n",
    "@btime solve_factoring($A, $B)\n",
    "\n",
    "# even better, use built-in feature for multiple RHS\n",
    "@btime $A \\ $B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangular Matrices and Back/Forward Substitution\n",
    "\n",
    "Some matrices are already in a convenient form and require no further factoring.\n",
    "\n",
    "For example, consider solving a system with an `UpperTriangular` matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 UpperTriangular{Float64,Array{Float64,2}}:\n",
       " 1.0  2.0  3.0\n",
       "  ⋅   5.0  6.0\n",
       "  ⋅    ⋅   9.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [1.0, 2.0, 3.0]\n",
    "U = UpperTriangular([1.0 2.0 3.0; 0.0 5.0 6.0; 0.0 0.0 9.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system is especially easy to solve using [back-substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution).  In particular, $ x_3 = b_3 / U_{33}, x_2 = (b_2 - x_3 U_{23})/U_{22} $, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.3333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `LowerTriangular` has similar properties and can be solved with forward-substitution.\n",
    "\n",
    "The computational order of back-substitution and forward-substitution is $ O(N^2) $ for dense matrices.  Those fast algorithms are a key reason that factorizaitons target triangular structures.\n",
    "\n",
    "\n",
    "<a id='jl-decomposition'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU Decomposition\n",
    "\n",
    "The $ LU $ decompositions finds a lower triangular $ L $ and upper triangular $ U $ such that $ L U = A $.\n",
    "\n",
    "For a general dense matrix without any other structure (i.e. not known to be symmetric, tridiagonal, etc.) this is the standard approach to solve a system and exploit the speed of backward and forward substitution using the factorization.\n",
    "\n",
    "The computational order of LU decomposition itself for a dense matrix is $ O(N^3) $ - the same as Gaussian elimination, but it tends\n",
    "to have a better constant term than others (e.g. half the number of operations of the QR Decomposition).  For structured\n",
    "or sparse matrices, that order drops.\n",
    "\n",
    "We can see which algorithm Julia will use for the `\\` operator by looking at the `factorize` function for a given\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64,Array{Float64,2}}\n",
       "L factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 1.0       0.0       0.0       0.0\n",
       " 0.563082  1.0       0.0       0.0\n",
       " 0.730109  0.912509  1.0       0.0\n",
       " 0.114765  0.227879  0.115228  1.0\n",
       "U factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 0.79794  0.28972   0.765939   0.496278\n",
       " 0.0      0.82524   0.23962   -0.130989\n",
       " 0.0      0.0      -0.447888   0.374303\n",
       " 0.0      0.0       0.0        0.725264"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "A = rand(N,N)\n",
    "b = rand(N)\n",
    "\n",
    "Af = factorize(A)  # chooses the right factorization, LU here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it provides an $ L $ and $ U $ factorization (with [pivoting](https://en.wikipedia.org/wiki/LU_decomposition#LU_factorization_with_full_pivoting) ).\n",
    "\n",
    "With the factorization complete, we can solve different `b` right hand sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " -0.4984260549573151\n",
       " -0.11835721499695559\n",
       "  1.5055538550184813\n",
       "  0.07694455957797534"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Af \\ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " -0.6456780666059366\n",
       " -0.26015157376547593\n",
       "  1.1168895662966312\n",
       "  0.5405293106660054"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = rand(N)\n",
    "Af \\ b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the decomposition also includes a $ P $ is a [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix) such\n",
    "that $ P A = L U $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Af.P * A ≈ Af.L * Af.U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly calculate an `lu` decomposition without the pivoting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64,Array{Float64,2}}\n",
       "L factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 1.0       0.0       0.0       0.0\n",
       " 0.730109  1.0       0.0       0.0\n",
       " 0.563082  1.09588   1.0       0.0\n",
       " 0.114765  0.249728  0.122733  1.0\n",
       "U factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 0.79794  0.28972    0.765939   0.496278\n",
       " 0.0      0.753039  -0.229233   0.254774\n",
       " 0.0      0.0        0.490832  -0.410191\n",
       " 0.0      0.0        0.0        0.725264"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, U = lu(A, Val(false))  # the Val(false) provides solution without permutation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify the decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A ≈ L * U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see roughly how the solver works, note that we can write the problem $ A x = b $ as $ L U x = b $.  Let $ U x = y $, which breaks the\n",
    "problem into two sub-problems.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L y &= b\\\\\n",
    "U x &= y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As we saw above, this is the solution to two triangular systems, which can be efficiently done with back or forward substitution in $ O(N^2) $ operations.\n",
    "\n",
    "To demonstrate this, first using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  0.759344042755733\n",
       " -0.41464678155905965\n",
       "  0.707411438334498\n",
       "  0.05580508465599854"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = L \\ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = U \\ y\n",
    "x ≈ A \\ b  # Check identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LU decomposition also has specialized algorithms for structured matrices, such as a `Tridiagonal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64,Tridiagonal{Float64,Array{Float64,1}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "b = rand(N)\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])\n",
    "factorize(A) |> typeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This factorization is the key to the performance of the `A \\ b` in this case.  For Tridiagonal matrices, the\n",
    "LU decomposition is $ O(N^2) $.\n",
    "\n",
    "Finally, just as a dense matrix without any structure use an LU decomposition to solve a system,\n",
    "so will the sparse solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuiteSparse.UMFPACK.UmfpackLU{Float64,Int64}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sparse = sparse(A)\n",
    "factorize(A_sparse) |> typeof  # dropping the tridiagonal structure to just become sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\\b for typeof(A) = Tridiagonal{Float64,Array{Float64,1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19.142 μs (9 allocations: 47.75 KiB)\n",
      "A\\b for typeof(A) = SparseMatrixCSC{Float64,Int64}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  605.311 μs (69 allocations: 1.06 MiB)\n"
     ]
    }
   ],
   "source": [
    "benchmark_solve(A, b)\n",
    "benchmark_solve(A_sparse, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sparsity, the computational order is related to the number of non-zeros rather than the size of the matrix itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "\n",
    "For real, symmetric, [positive semi-definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) matrices, a Cholesky decomposition is a specialized example of the LU decomposition where $ L = U' $.\n",
    "\n",
    "The Cholesky is directly useful on its own (e.g. [Classical Control with Linear Algebra](../time_series_models/classical_filtering.html)) but it is also an efficient factorization to solve symmetric positive semi-definite systems.\n",
    "\n",
    "As always, symmetry allows specialized algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BunchKaufman{Float64,Array{Float64,2}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 500\n",
    "B = rand(N,N)\n",
    "A_dense = B' * B  # an easy way to generate a symmetric positive semi-definite matrix\n",
    "A = Symmetric(A_dense)  # flags the matrix as symmetric\n",
    "\n",
    "factorize(A) |> typeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the $ A $ decomposition is [Bunch-Kaufman](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#LinearAlgebra.bunchkaufman) rather than a\n",
    "Cholesky, because Julia doesn’t know the matrix is positive semi-definite.  We can manually factorize with a Cholesky,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cholesky{Float64,Array{Float64,2}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cholesky(A) |> typeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\\b for typeof(A) = Symmetric{Float64,Array{Float64,2}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.307 ms (8 allocations: 2.16 MiB)\n",
      "A\\b for typeof(A) = Array{Float64,2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.121 ms (5 allocations: 1.92 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.634 ms (7 allocations: 1.91 MiB)\n"
     ]
    }
   ],
   "source": [
    "b = rand(N)\n",
    "cholesky(A) \\ b  # use the factorization to solve\n",
    "\n",
    "benchmark_solve(A, b)\n",
    "benchmark_solve(A_dense, b)\n",
    "@btime cholesky($A, check=false) \\ $b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "Previously, we learned about applications of the QR decomposition to solving the linear least squares.\n",
    "\n",
    "While in principle, the solution to the least-squares problem\n",
    "\n",
    "$$\n",
    "\\min_x \\| Ax -b \\|^2\n",
    "$$\n",
    "\n",
    "is $ x = (A'A)^{-1}A'b $, in practice note that $ A'A $ becomes dense and calculating the inverse is rarely a good idea.\n",
    "\n",
    "The QR decomposition is a decomposition $ A = Q R $ where $ Q $ is an orthogonal matrix (i.e. $ Q'Q = Q Q' = I $) and $ R $ is\n",
    "an upper triangular matrix.\n",
    "\n",
    "Given the  previous derivation we showed that we can write the least squares problem as\n",
    "the solution to\n",
    "\n",
    "$$\n",
    "R x = Q' b\n",
    "$$\n",
    "\n",
    "Where, as discussed above, the upper-triangular structure of $ R $ can be solved easily with back substitution.\n",
    "\n",
    "The `\\` operator solves the linear-least squares problem whenever the given `A` is rectangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.4011747124872587\n",
       "  0.07361080010718472\n",
       " -0.23478068012724587"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "M = 3\n",
    "x_true = rand(3)\n",
    "\n",
    "A = rand(N,M) .+ randn(N)\n",
    "b = rand(N)\n",
    "x = A \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually use the QR decomposition in solving linear least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q * R ≈ A = true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.4011747124872585\n",
       "  0.07361080010718468\n",
       " -0.23478068012724573"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Af = qr(A)\n",
    "Q = Af.Q\n",
    "R = [Af.R; zeros(N - M, M)] # Stack with zeros\n",
    "@show Q * R ≈ A\n",
    "x = R \\ Q'*b  # simplified QR solution for least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stacks the `R` with zeros, but the more specialized algorithm would not multiply directly\n",
    "in that way.\n",
    "\n",
    "In some cases, if an LU is not available for a particular matrix structure, the QR factorization\n",
    "can also be used to solve systems of equations (i.e. not just LLS).  This tends to be about 2x slower than the LU,\n",
    "but is of the same computational order.\n",
    "\n",
    "Deriving the approach, where we can now use inverse since the system is square and we assumed $ A $ was non-singular\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A x &= b\\\\\n",
    "Q R x &= b\\\\\n",
    "Q^{-1} Q R x &= Q^{-1} b\\\\\n",
    "R x &= Q' b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the last step uses that $ Q^{-1} = Q' $ for an orthogonal matrix.\n",
    "\n",
    "Given the decomposition, the solution for dense matrices is of computational\n",
    "order $ O(N^2) $.  To see this, look at the order of each operation.\n",
    "\n",
    "- Since $ R $ is upper-triangular matrix, it can be solved quickly through back substitution with computational order $ O(N^2) $  \n",
    "- A transpose operation is of order $ O(N^2) $  \n",
    "- A matrix-vector product is also $ O(N^2) $  \n",
    "\n",
    "\n",
    "In all cases, the order would drop depending on the sparsity pattern of the\n",
    "matrix (and corresponding decomposition).  A key benefit of a QR decomposition is that it tends to\n",
    "maintain sparsity.\n",
    "\n",
    "Without implementing the full process, you can form a QR\n",
    "factorization with `qr` and then use it to solve a system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \\ b = [-1.4780409419445582, 2.0987575263439306, -0.6857071090150311, -0.16849538664184538, 2.012803045177841]\n",
      "qr(A) \\ b = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.478040941944558, 2.09875752634393, -0.6857071090150318, -0.16849538664184394, 2.012803045177841]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "A = rand(N,N)\n",
    "b = rand(N)\n",
    "@show A \\ b\n",
    "@show qr(A) \\ b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Decomposition\n",
    "\n",
    "A spectral decomposition, also known as an [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix), finds all of the eigenvectors and eigenvalues to decompose a square matrix `A` such that\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^{-1}\n",
    "$$\n",
    "\n",
    "where $ Q $ is a matrix made of the the eigenvectors of $ A $ as columns, and $ \\Lambda $ is a diagonal matrix of the eigenvalues.  Only square, [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix) matrices have an eigendecomposition (where a matrix is not diagonalizable if it does not have a full set of linearly independent eigenvectors).\n",
    "\n",
    "In Julia, whenever you ask for a full set of eigenvectors and eigenvalues, it  decomposes using an algorithm appropriate for the matrix type.  For example, symmetric, hermitian, or tridiagonal matrices have specialized algorithms.\n",
    "\n",
    "To see this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.672829120462253e-15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Symmetric(rand(5, 5))  # symmetric matrices have real eigenvectors/eigenvalues\n",
    "A_eig = eigen(A)\n",
    "Λ = Diagonal(A_eig.values)\n",
    "Q = A_eig.vectors\n",
    "norm(Q * Λ * inv(Q) - A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that a real matrix may have complex eigenvalues and eigenvectors, so if you attempt  to check `Q * Λ * inv(Q) - A` - even for a positive-definite matrix - it may not be a real number due to numerical inaccuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Time Markov Chains (CTMC)\n",
    "\n",
    "In the previous lecture on discrete time Markov chains, we saw that the transition probability\n",
    "between state $ x $ and state $ y $ was summarized by the matrix $ P(x, y) := \\mathbb P \\{ X_{t+1} = y \\,|\\, X_t = x \\} $.\n",
    "\n",
    "As a brief introduction to continuous time processes, consider the same state-space as in the discrete\n",
    "case: $ S $ a finite set with $ n $ elements $ \\{x_1, \\ldots, x_n\\} $.\n",
    "\n",
    "A **Markov chain** $ \\{X_t\\} $ on $ S $ is a sequence of random variables on $ S $ that have the **Markov property**.\n",
    "\n",
    "In continuous time, the [Markov Property](https://en.wikipedia.org/wiki/Markov_property) is more complicated, but intuitively is\n",
    "the same as the discrete time case.\n",
    "\n",
    "That is, knowing the current state is enough to know probabilities for future states.  Or, for realizations $ x(\\tau)\\in S, \\tau \\leq t $,\n",
    "\n",
    "$$\n",
    "\\mathbb P \\{ X(t+s) = y  \\,|\\, X(t) = x, X(\\tau) = x(\\tau) \\text{ for } 0 \\leq \\tau \\leq t  \\} = \\mathbb P \\{ X(t+s) = y  \\,|\\, X(t) = x\\}\n",
    "$$\n",
    "\n",
    "Heuristically, consider a time period $ t $ and a small step forward $ \\Delta $.  Then the probability to transition from state $ i $ to\n",
    "state $ j $ is\n",
    "\n",
    "$$\n",
    "\\mathbb P \\{ X(t + \\Delta) = j  \\,|\\, X(t) \\} = \\begin{cases} q_{ij} \\Delta + o(\\Delta) & i \\neq j\\\\\n",
    "                                                              1 + q_{ii} \\Delta + o(\\Delta) & i = j \\end{cases}\n",
    "$$\n",
    "\n",
    "where $ q_{ij} $ are “intensity” parameters governing the transition rate, and $ o(\\Delta) $ is [little-o notation](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation).  That is, $ \\lim_{\\Delta\\to 0} o(\\Delta)/\\Delta = 0 $.\n",
    "\n",
    "Just as in the discrete case, we can summarize these parameters by a $ N \\times N $ matrix, $ Q \\in R^{N\\times N} $.\n",
    "\n",
    "Recall that in the discrete case every element is weakly positive and every row must sum to one.   Instead, with a continuous time the rows of $ Q $ sum to zero, where the diagonal contains the negative value of jumping out of the current state.  That is\n",
    "\n",
    "- $ q_{ij} \\geq 0 $ for $ i \\neq j $  \n",
    "- $ q_{ii} \\leq 0 $  \n",
    "- $ \\sum_{j} q_{ij} = 0 $  \n",
    "\n",
    "\n",
    "The $ Q $ matrix is called the intensity matrix, or the infinitesimal generator of the Markov chain.  For example,\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} -0.1 & 0.1  & 0 & 0 & 0 & 0\\\\\n",
    "                    0.1  &-0.2  & 0.1 &  0 & 0 & 0\\\\\n",
    "                    0 & 0.1 & -0.2 & 0.1 & 0 & 0\\\\\n",
    "                    0 & 0 & 0.1 & -0.2 & 0.1 & 0\\\\\n",
    "                    0 & 0 & 0 & 0.1 & -0.2 & 0.1\\\\\n",
    "                    0 & 0 & 0 & 0 & 0.1 & -0.1\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the above example, transitions only occur between adjacent states with the same intensity (except for a ``bouncing’’ back of the bottom and top states).\n",
    "\n",
    "Implementing the $ Q $ using its tridiagonal structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " -0.1   0.1    ⋅     ⋅     ⋅     ⋅ \n",
       "  0.1  -0.2   0.1    ⋅     ⋅     ⋅ \n",
       "   ⋅    0.1  -0.2   0.1    ⋅     ⋅ \n",
       "   ⋅     ⋅    0.1  -0.2   0.1    ⋅ \n",
       "   ⋅     ⋅     ⋅    0.1  -0.2   0.1\n",
       "   ⋅     ⋅     ⋅     ⋅    0.1  -0.1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "α = 0.1\n",
    "N = 6\n",
    "Q = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use the `Tridiagonal` to exploit the structure of the problem.\n",
    "\n",
    "Consider a simple payoff vector $ r $ associated with each state, and a discount rate $ ρ $.  Then we can solve for\n",
    "the expected present discounted value in a similar way to the discrete time case.\n",
    "\n",
    "$$\n",
    "\\rho v = r + Q v\n",
    "$$\n",
    "\n",
    "or rearranging slightly, solving the linear system\n",
    "\n",
    "$$\n",
    "(\\rho I - Q) v = r\n",
    "$$\n",
    "\n",
    "For our example, exploiting the tridiagonal structure,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 Tridiagonal{Float64,Array{Float64,1}}:\n",
       "  0.15  -0.1     ⋅      ⋅      ⋅      ⋅ \n",
       " -0.1    0.25  -0.1     ⋅      ⋅      ⋅ \n",
       "   ⋅    -0.1    0.25  -0.1     ⋅      ⋅ \n",
       "   ⋅      ⋅    -0.1    0.25  -0.1     ⋅ \n",
       "   ⋅      ⋅      ⋅    -0.1    0.25  -0.1\n",
       "   ⋅      ⋅      ⋅      ⋅    -0.1    0.15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(0.0, 10.0, length=N)\n",
    "ρ = 0.05\n",
    "\n",
    "A = ρ * I - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this $ A $ matrix is maintaining the tridiagonal structure of the problem, which leads to an efficient solution to the\n",
    "linear problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       "  38.15384615384615\n",
       "  57.23076923076923\n",
       "  84.92307692307693\n",
       " 115.07692307692311\n",
       " 142.76923076923077\n",
       " 161.84615384615384"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = A \\ r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $ Q $ is also used to calculate the evolution of the Markov chain, in direct analogy to the $ ψ_{t+k} = ψ_t P^k $ evolution with transition matrix $ P $ of the discrete case.\n",
    "\n",
    "In the continuous case, this becomes the system of linear differential equations\n",
    "\n",
    "$$\n",
    "\\dot{ψ}(t) = Q(t)^T ψ(t)\n",
    "$$\n",
    "\n",
    "given the initial condition $ \\psi(0) $ and where the $ Q(t) $ intensity matrix is allows to vary with time.  In the simplest case of a constant $ Q $ matrix, this is a simple constant-coefficient system of linear ODEs with coefficients $ Q^T $.\n",
    "\n",
    "If a stationary equilibria exists, note that $ \\dot{ψ}(t) = 0 $, and the stationary solution $ ψ^{*} $ needs to fulfill\n",
    "\n",
    "$$\n",
    "0 = Q^T ψ^{*}\n",
    "$$\n",
    "\n",
    "Notice that this is of the form $ 0 ψ^{*} = Q^T ψ^{*} $ and hence is equivalent to finding the eigenvector associated with the $ \\lambda = 0 $ eigenvalue of $ Q^T $.\n",
    "\n",
    "With our example, we can calculate all of the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64,Float64,Array{Float64,2},Array{Float64,1}}\n",
       "values:\n",
       "6-element Array{Float64,1}:\n",
       " -0.3732050807568874\n",
       " -0.29999999999999993\n",
       " -0.19999999999999998\n",
       " -0.09999999999999995\n",
       " -0.026794919243112274\n",
       "  0.0\n",
       "vectors:\n",
       "6×6 Array{Float64,2}:\n",
       " -0.149429  -0.288675   0.408248   0.5          -0.557678  0.408248\n",
       "  0.408248   0.57735   -0.408248   1.38778e-16  -0.408248  0.408248\n",
       " -0.557678  -0.288675  -0.408248  -0.5          -0.149429  0.408248\n",
       "  0.557678  -0.288675   0.408248  -0.5           0.149429  0.408248\n",
       " -0.408248   0.57735    0.408248   7.63278e-16   0.408248  0.408248\n",
       "  0.149429  -0.288675  -0.408248   0.5           0.557678  0.408248"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "λ, vecs = eigen(Array(Q'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there is a $ \\lambda = 0 $ eigenvalue, which is associated with the last column in the eigenvector.  To turn that into a probability\n",
    "we need to normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       " 0.16666666666666657\n",
       " 0.16666666666666657\n",
       " 0.1666666666666667\n",
       " 0.16666666666666682\n",
       " 0.16666666666666685\n",
       " 0.16666666666666663"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[:,N] ./ sum(vecs[:,N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Dimensions\n",
    "\n",
    "A frequent case in discretized models is dealing with Markov chains with multiple “spatial” dimensions (e.g. wealth and income).\n",
    "\n",
    "After discretizing a process to create a Markov chain, you can always take the cartesian product of the set of states in order to\n",
    "enumerate as a single state variable.\n",
    "\n",
    "To see this, consider states $ i $ and $ j $ governed by infinitesimal generators $ Q $ and $ A $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Array{Float64,2}:\n",
       " -0.2   0.1   0.0   0.0   0.1   0.0   0.0   0.0\n",
       "  0.1  -0.3   0.1   0.0   0.0   0.1   0.0   0.0\n",
       "  0.0   0.1  -0.3   0.1   0.0   0.0   0.1   0.0\n",
       "  0.0   0.0   0.1  -0.2   0.0   0.0   0.0   0.1\n",
       "  0.2   0.0   0.0   0.0  -0.3   0.1   0.0   0.0\n",
       "  0.0   0.2   0.0   0.0   0.1  -0.4   0.1   0.0\n",
       "  0.0   0.0   0.2   0.0   0.0   0.1  -0.4   0.1\n",
       "  0.0   0.0   0.0   0.2   0.0   0.0   0.1  -0.3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function markov_chain_product(Q, A)\n",
    "    M = size(Q, 1)\n",
    "    N = size(A, 1)\n",
    "    Q = sparse(Q)\n",
    "    Qs = blockdiag(fill(Q, N)...)  # create diagonal blocks of every operator\n",
    "    As = kron(A, sparse(I(M)))\n",
    "    return As + Qs\n",
    "end\n",
    "\n",
    "α = 0.1\n",
    "N = 4\n",
    "Q = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))\n",
    "A = sparse([-0.1 0.1\n",
    "    0.2 -0.2])\n",
    "M = size(A,1)\n",
    "L = markov_chain_product(Q, A)\n",
    "L |> Matrix  # display as a dense matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the combined markov chain for the $ (i,j) $ process.  To see the sparsity pattern,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Module JSON with build ID 359299871038753 is missing from the cache.\n",
      "│ This may mean JSON [682c06a0-de6a-54ab-a142-c8b1cf79cde6] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:1016\n",
      "┌ Info: Skipping precompilation since __precompile__(false). Importing Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80].\n",
      "└ @ Base loading.jl:1033\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip740\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip740)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip741\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip740)\" d=\"\n",
       "M392.865 1486.45 L1832.07 1486.45 L1832.07 47.2441 L392.865 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip742\">\n",
       "    <rect x=\"392\" y=\"47\" width=\"1440\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,1486.45 1832.07,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,47.2441 392.865,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,1486.45 392.865,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  598.465,1486.45 598.465,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  804.066,1486.45 804.066,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1009.67,1486.45 1009.67,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1215.27,1486.45 1215.27,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1420.87,1486.45 1420.87,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1626.47,1486.45 1626.47,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1832.07,1486.45 1832.07,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,47.2441 410.135,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,252.845 410.135,252.845 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,458.445 410.135,458.445 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,664.046 410.135,664.046 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,869.646 410.135,869.646 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,1075.25 410.135,1075.25 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,1280.85 410.135,1280.85 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.865,1486.45 410.135,1486.45 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip740)\" d=\"M 0 0 M383.247 1535.98 L390.885 1535.98 L390.885 1509.62 L382.575 1511.29 L382.575 1507.03 L390.839 1505.36 L395.515 1505.36 L395.515 1535.98 L403.154 1535.98 L403.154 1539.92 L383.247 1539.92 L383.247 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M593.118 1535.98 L609.437 1535.98 L609.437 1539.92 L587.493 1539.92 L587.493 1535.98 Q590.155 1533.23 594.738 1528.6 Q599.345 1523.95 600.525 1522.61 Q602.771 1520.08 603.65 1518.35 Q604.553 1516.59 604.553 1514.9 Q604.553 1512.14 602.609 1510.41 Q600.687 1508.67 597.586 1508.67 Q595.387 1508.67 592.933 1509.43 Q590.502 1510.2 587.725 1511.75 L587.725 1507.03 Q590.549 1505.89 593.002 1505.31 Q595.456 1504.73 597.493 1504.73 Q602.863 1504.73 606.058 1507.42 Q609.252 1510.11 609.252 1514.6 Q609.252 1516.73 608.442 1518.65 Q607.655 1520.54 605.548 1523.14 Q604.97 1523.81 601.868 1527.03 Q598.766 1530.22 593.118 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M808.313 1521.29 Q811.67 1522 813.545 1524.27 Q815.443 1526.54 815.443 1529.87 Q815.443 1534.99 811.924 1537.79 Q808.406 1540.59 801.925 1540.59 Q799.749 1540.59 797.434 1540.15 Q795.142 1539.73 792.688 1538.88 L792.688 1534.36 Q794.633 1535.5 796.948 1536.08 Q799.263 1536.66 801.786 1536.66 Q806.184 1536.66 808.475 1534.92 Q810.79 1533.18 810.79 1529.87 Q810.79 1526.82 808.637 1525.11 Q806.508 1523.37 802.688 1523.37 L798.661 1523.37 L798.661 1519.53 L802.874 1519.53 Q806.323 1519.53 808.151 1518.16 Q809.98 1516.77 809.98 1514.18 Q809.98 1511.52 808.082 1510.11 Q806.207 1508.67 802.688 1508.67 Q800.767 1508.67 798.568 1509.09 Q796.369 1509.5 793.73 1510.38 L793.73 1506.22 Q796.392 1505.48 798.707 1505.11 Q801.045 1504.73 803.105 1504.73 Q808.429 1504.73 811.531 1507.17 Q814.633 1509.57 814.633 1513.69 Q814.633 1516.56 812.989 1518.55 Q811.346 1520.52 808.313 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1012.68 1509.43 L1000.87 1527.88 L1012.68 1527.88 L1012.68 1509.43 M1011.45 1505.36 L1017.33 1505.36 L1017.33 1527.88 L1022.26 1527.88 L1022.26 1531.77 L1017.33 1531.77 L1017.33 1539.92 L1012.68 1539.92 L1012.68 1531.77 L997.074 1531.77 L997.074 1527.26 L1011.45 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1205.54 1505.36 L1223.9 1505.36 L1223.9 1509.3 L1209.83 1509.3 L1209.83 1517.77 Q1210.85 1517.42 1211.86 1517.26 Q1212.88 1517.07 1213.9 1517.07 Q1219.69 1517.07 1223.07 1520.24 Q1226.45 1523.42 1226.45 1528.83 Q1226.45 1534.41 1222.98 1537.51 Q1219.5 1540.59 1213.18 1540.59 Q1211.01 1540.59 1208.74 1540.22 Q1206.49 1539.85 1204.09 1539.11 L1204.09 1534.41 Q1206.17 1535.54 1208.39 1536.1 Q1210.61 1536.66 1213.09 1536.66 Q1217.1 1536.66 1219.43 1534.55 Q1221.77 1532.44 1221.77 1528.83 Q1221.77 1525.22 1219.43 1523.11 Q1217.1 1521.01 1213.09 1521.01 Q1211.22 1521.01 1209.34 1521.42 Q1207.49 1521.84 1205.54 1522.72 L1205.54 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1421.27 1520.78 Q1418.12 1520.78 1416.27 1522.93 Q1414.44 1525.08 1414.44 1528.83 Q1414.44 1532.56 1416.27 1534.73 Q1418.12 1536.89 1421.27 1536.89 Q1424.42 1536.89 1426.25 1534.73 Q1428.1 1532.56 1428.1 1528.83 Q1428.1 1525.08 1426.25 1522.93 Q1424.42 1520.78 1421.27 1520.78 M1430.55 1506.12 L1430.55 1510.38 Q1428.8 1509.55 1426.99 1509.11 Q1425.21 1508.67 1423.45 1508.67 Q1418.82 1508.67 1416.37 1511.8 Q1413.93 1514.92 1413.59 1521.24 Q1414.95 1519.23 1417.01 1518.16 Q1419.07 1517.07 1421.55 1517.07 Q1426.76 1517.07 1429.77 1520.24 Q1432.8 1523.39 1432.8 1528.83 Q1432.8 1534.16 1429.65 1537.37 Q1426.5 1540.59 1421.27 1540.59 Q1415.28 1540.59 1412.11 1536.01 Q1408.93 1531.4 1408.93 1522.67 Q1408.93 1514.48 1412.82 1509.62 Q1416.71 1504.73 1423.26 1504.73 Q1425.02 1504.73 1426.8 1505.08 Q1428.61 1505.43 1430.55 1506.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1615.36 1505.36 L1637.58 1505.36 L1637.58 1507.35 L1625.03 1539.92 L1620.15 1539.92 L1631.95 1509.3 L1615.36 1509.3 L1615.36 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1832.07 1523.51 Q1828.74 1523.51 1826.81 1525.29 Q1824.92 1527.07 1824.92 1530.2 Q1824.92 1533.32 1826.81 1535.11 Q1828.74 1536.89 1832.07 1536.89 Q1835.4 1536.89 1837.32 1535.11 Q1839.24 1533.3 1839.24 1530.2 Q1839.24 1527.07 1837.32 1525.29 Q1835.42 1523.51 1832.07 1523.51 M1827.39 1521.52 Q1824.38 1520.78 1822.69 1518.72 Q1821.03 1516.66 1821.03 1513.69 Q1821.03 1509.55 1823.97 1507.14 Q1826.93 1504.73 1832.07 1504.73 Q1837.23 1504.73 1840.17 1507.14 Q1843.11 1509.55 1843.11 1513.69 Q1843.11 1516.66 1841.42 1518.72 Q1839.75 1520.78 1836.77 1521.52 Q1840.15 1522.3 1842.02 1524.6 Q1843.92 1526.89 1843.92 1530.2 Q1843.92 1535.22 1840.84 1537.91 Q1837.79 1540.59 1832.07 1540.59 Q1826.35 1540.59 1823.27 1537.91 Q1820.22 1535.22 1820.22 1530.2 Q1820.22 1526.89 1822.11 1524.6 Q1824.01 1522.3 1827.39 1521.52 M1825.68 1514.13 Q1825.68 1516.82 1827.35 1518.32 Q1829.04 1519.83 1832.07 1519.83 Q1835.08 1519.83 1836.77 1518.32 Q1838.48 1516.82 1838.48 1514.13 Q1838.48 1511.45 1836.77 1509.94 Q1835.08 1508.44 1832.07 1508.44 Q1829.04 1508.44 1827.35 1509.94 Q1825.68 1511.45 1825.68 1514.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M348.957 60.5889 L356.596 60.5889 L356.596 34.2233 L348.286 35.89 L348.286 31.6308 L356.55 29.9641 L361.226 29.9641 L361.226 60.5889 L368.865 60.5889 L368.865 64.5241 L348.957 64.5241 L348.957 60.5889 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M352.545 266.189 L368.865 266.189 L368.865 270.125 L346.92 270.125 L346.92 266.189 Q349.582 263.435 354.166 258.805 Q358.772 254.152 359.953 252.81 Q362.198 250.287 363.078 248.551 Q363.98 246.791 363.98 245.102 Q363.98 242.347 362.036 240.611 Q360.115 238.875 357.013 238.875 Q354.814 238.875 352.36 239.639 Q349.93 240.403 347.152 241.953 L347.152 237.231 Q349.976 236.097 352.43 235.518 Q354.883 234.94 356.92 234.94 Q362.291 234.94 365.485 237.625 Q368.679 240.31 368.679 244.801 Q368.679 246.93 367.869 248.852 Q367.082 250.75 364.976 253.342 Q364.397 254.014 361.295 257.231 Q358.193 260.426 352.545 266.189 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M361.735 457.091 Q365.092 457.809 366.967 460.077 Q368.865 462.346 368.865 465.679 Q368.865 470.795 365.346 473.596 Q361.828 476.396 355.346 476.396 Q353.17 476.396 350.855 475.957 Q348.564 475.54 346.11 474.684 L346.11 470.17 Q348.055 471.304 350.369 471.883 Q352.684 472.461 355.207 472.461 Q359.605 472.461 361.897 470.725 Q364.212 468.989 364.212 465.679 Q364.212 462.623 362.059 460.91 Q359.93 459.174 356.11 459.174 L352.082 459.174 L352.082 455.332 L356.295 455.332 Q359.744 455.332 361.573 453.966 Q363.402 452.577 363.402 449.985 Q363.402 447.323 361.504 445.911 Q359.629 444.475 356.11 444.475 Q354.189 444.475 351.99 444.892 Q349.791 445.309 347.152 446.188 L347.152 442.022 Q349.814 441.281 352.129 440.911 Q354.467 440.54 356.527 440.54 Q361.851 440.54 364.953 442.971 Q368.054 445.378 368.054 449.498 Q368.054 452.369 366.411 454.36 Q364.767 456.327 361.735 457.091 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M359.281 650.84 L347.476 669.289 L359.281 669.289 L359.281 650.84 M358.055 646.766 L363.934 646.766 L363.934 669.289 L368.865 669.289 L368.865 673.178 L363.934 673.178 L363.934 681.326 L359.281 681.326 L359.281 673.178 L343.68 673.178 L343.68 668.664 L358.055 646.766 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M347.962 852.366 L366.318 852.366 L366.318 856.301 L352.244 856.301 L352.244 864.774 Q353.263 864.426 354.281 864.264 Q355.3 864.079 356.318 864.079 Q362.105 864.079 365.485 867.25 Q368.865 870.422 368.865 875.838 Q368.865 881.417 365.392 884.519 Q361.92 887.598 355.601 887.598 Q353.425 887.598 351.156 887.227 Q348.911 886.857 346.504 886.116 L346.504 881.417 Q348.587 882.551 350.809 883.107 Q353.031 883.662 355.508 883.662 Q359.513 883.662 361.851 881.556 Q364.189 879.449 364.189 875.838 Q364.189 872.227 361.851 870.121 Q359.513 868.014 355.508 868.014 Q353.633 868.014 351.758 868.431 Q349.906 868.848 347.962 869.727 L347.962 852.366 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M357.337 1073.38 Q354.189 1073.38 352.337 1075.54 Q350.508 1077.69 350.508 1081.44 Q350.508 1085.17 352.337 1087.34 Q354.189 1089.49 357.337 1089.49 Q360.485 1089.49 362.314 1087.34 Q364.166 1085.17 364.166 1081.44 Q364.166 1077.69 362.314 1075.54 Q360.485 1073.38 357.337 1073.38 M366.619 1058.73 L366.619 1062.99 Q364.86 1062.16 363.054 1061.72 Q361.272 1061.28 359.513 1061.28 Q354.883 1061.28 352.43 1064.4 Q349.999 1067.53 349.652 1073.85 Q351.018 1071.83 353.078 1070.77 Q355.138 1069.68 357.615 1069.68 Q362.823 1069.68 365.832 1072.85 Q368.865 1076 368.865 1081.44 Q368.865 1086.76 365.717 1089.98 Q362.568 1093.2 357.337 1093.2 Q351.342 1093.2 348.17 1088.61 Q344.999 1084.01 344.999 1075.28 Q344.999 1067.09 348.888 1062.23 Q352.777 1057.34 359.328 1057.34 Q361.087 1057.34 362.869 1057.69 Q364.675 1058.04 366.619 1058.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M346.643 1263.57 L368.865 1263.57 L368.865 1265.56 L356.318 1298.13 L351.434 1298.13 L363.24 1267.5 L346.643 1267.5 L346.643 1263.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M357.013 1487.32 Q353.68 1487.32 351.758 1489.1 Q349.86 1490.88 349.86 1494.01 Q349.86 1497.13 351.758 1498.91 Q353.68 1500.7 357.013 1500.7 Q360.346 1500.7 362.267 1498.91 Q364.189 1497.11 364.189 1494.01 Q364.189 1490.88 362.267 1489.1 Q360.369 1487.32 357.013 1487.32 M352.337 1485.33 Q349.328 1484.58 347.638 1482.52 Q345.971 1480.46 345.971 1477.5 Q345.971 1473.36 348.911 1470.95 Q351.874 1468.54 357.013 1468.54 Q362.175 1468.54 365.115 1470.95 Q368.054 1473.36 368.054 1477.5 Q368.054 1480.46 366.365 1482.52 Q364.698 1484.58 361.712 1485.33 Q365.092 1486.11 366.967 1488.4 Q368.865 1490.7 368.865 1494.01 Q368.865 1499.03 365.786 1501.71 Q362.73 1504.4 357.013 1504.4 Q351.295 1504.4 348.217 1501.71 Q345.161 1499.03 345.161 1494.01 Q345.161 1490.7 347.059 1488.4 Q348.957 1486.11 352.337 1485.33 M350.624 1477.94 Q350.624 1480.63 352.291 1482.13 Q353.98 1483.64 357.013 1483.64 Q360.022 1483.64 361.712 1482.13 Q363.425 1480.63 363.425 1477.94 Q363.425 1475.26 361.712 1473.75 Q360.022 1472.25 357.013 1472.25 Q353.98 1472.25 352.291 1473.75 Q350.624 1475.26 350.624 1477.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip742)\" cx=\"392.865\" cy=\"47.2441\" r=\"36\" fill=\"#781c6d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"392.865\" cy=\"252.845\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"392.865\" cy=\"869.646\" r=\"36\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"598.465\" cy=\"47.2441\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"598.465\" cy=\"252.845\" r=\"36\" fill=\"#33095e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"598.465\" cy=\"458.445\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"598.465\" cy=\"1075.25\" r=\"36\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"804.066\" cy=\"252.845\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"804.066\" cy=\"458.445\" r=\"36\" fill=\"#33095e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"804.066\" cy=\"664.046\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"804.066\" cy=\"1280.85\" r=\"36\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1009.67\" cy=\"458.445\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1009.67\" cy=\"664.046\" r=\"36\" fill=\"#781c6d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1009.67\" cy=\"1486.45\" r=\"36\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1215.27\" cy=\"47.2441\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1215.27\" cy=\"869.646\" r=\"36\" fill=\"#33095e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1215.27\" cy=\"1075.25\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1420.87\" cy=\"252.845\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1420.87\" cy=\"869.646\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1420.87\" cy=\"1075.25\" r=\"36\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1420.87\" cy=\"1280.85\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1626.47\" cy=\"458.445\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1626.47\" cy=\"1075.25\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1626.47\" cy=\"1280.85\" r=\"36\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1626.47\" cy=\"1486.45\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1832.07\" cy=\"664.046\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1832.07\" cy=\"1280.85\" r=\"36\" fill=\"#fbb419\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<circle clip-path=\"url(#clip742)\" cx=\"1832.07\" cy=\"1486.45\" r=\"36\" fill=\"#33095e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"0\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip743\">\n",
       "    <rect x=\"1880\" y=\"47\" width=\"73\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip743)\">\n",
       "<image width=\"72\" height=\"1439\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAWfCAYAAAAI7rB3AAAMWUlEQVR4nO3dwY0rRxAFwaZQ/lsh\n",
       "L6XfLQtUeSQPERYsEg8c9MyQ+/n3/v0O/+uvb/8Bv06gIFAQKAgU5r5/v/03/DQLCgIFgYJAQaAw\n",
       "7/359t/w0ywoCBQECgIFR41gQUGgIFAQKAgU5rmKrSwoCBQECgIFgcK86yq2saAgUBAoCBQcNYIF\n",
       "BYGCQEGgIFBwFQsWFAQKAgWBgvtBwYKCQEGgIFAQKMxx1FhZUBAoCBQECgIFN8yCBQWBgkBBoDDH\n",
       "DbOVBQWBgkBBoCBQmHP/+fbf8NMsKAgUBAoCBYGCxz7BgoJAQaAgUHDDLFhQECgIFAQKAgVXsWBB\n",
       "QaAgUBAouB8ULCgIFAQKAgWBwnwcNVYWFAQKAgWBgkBhzvWfWTYWFAQKAgWBgqNGsKAgUBAoCBQE\n",
       "Co4awYKCQEGgIFAQKHjDLFhQECgIFAQK83HUWFlQECgIFAQKAgXvSQcLCgIFgYJAwVEjWFAQKAgU\n",
       "BAoCBc/mgwUFgYJAQaAgUHAWCxYUBAoCBYGCo0awoCBQECgIFAQKjhrBgoJAQaAgUBAozLn323/D\n",
       "T7OgIFAQKAgUHDWCBQWBgkBBoCBQ8NgnWFAQKAgUBAo+pIMFBYGCQEGgIFCYj6caKwsKAgWBgkBB\n",
       "oOAsFiwoCBQECgIFL1AFCwoCBYGCQEGg4IZZsKAgUBAoCBTcDwoWFAQKAgWBgkDBDbNgQUGgIFAQ\n",
       "KAgUXMWCBQWBgkBBoOB788GCgkBBoCBQECg4agQLCgIFgYJAQaDgKhYsKAgUBAoCBR/SwYKCQEGg\n",
       "IFAQKLiKBQsKAgWBgkBhzvMhvbGgIFAQKAgUBAqOGsGCgkBBoCBQECi4igULCgIFgYJAwYd0sKAg\n",
       "UBAoCBQECnPu+/bf8NMsKAgUBAoCBYGCs1iwoCBQECgIFBw1ggUFgYJAQaAgUHDUCBYUBAoCBYGC\n",
       "o0awoCBQECgIFAQKc5w0VhYUBAoCBYGCQGHOcxbbWFAQKAgUBAqOGsGCgkBBoCBQECi4igULCgIF\n",
       "gYJAQaAwfohzZ0FBoCBQECg4agQLCgIFgYJAQaDgKhYsKAgUBAoChTn38+2/4adZUBAoCBQECgKF\n",
       "ec9VbGNBQaAgUBAoCBTcMAsWFAQKAgWBghtmwYKCQEGgIFAQKMy7Gm3UCQIFgYJAwVEjWFAQKAgU\n",
       "BAoChTmeza8sKAgUBAoCBYHCPGexlQUFgYJAQaAwx1ONlTpBoCBQECgIFBw1ggUFgYJAQaAgUPDY\n",
       "J1hQECgIFAQKXgMO6gSBgkBBoCBQ8J50sKAgUBAoCBQ81QgWFAQKAgWBgkBh3tNoo04QKAgUBAoC\n",
       "BTfMggUFgYJAQaDghlmwoCBQECgIFAQKc9wwW6kTBAoCBYGCQMFZLFhQECgIFAQKPqSDBQWBgkBB\n",
       "oCBQ8L35YEFBoCBQECj43nxQJwgUBAoCBYHCPEeNlQUFgYJAQaAgUPDYJ1hQECgIFAQK/jNLUCcI\n",
       "FAQKAgWBghtmwYKCQEGgIFAQKLhhFiwoCBQECgIFR41gQUGgIFAQKAgUvCcd1AkCBYGCQMFRI1hQ\n",
       "ECgIFAQKAgVPNYIFBYGCQEGgIFBwFgsWFAQKAgWBgn+fFSwoCBQECgIFgYJ/ZRzUCQIFgYJAQaDg\n",
       "hlmwoCBQECgIFDybDxYUBAoCBYGCQMFRI1hQECgIFAQKc31IrywoCBQECgIFgYLvzQd1gkBBoCBQ\n",
       "ECi4YRYsKAgUBAoCBR/SwYKCQEGgIFAQKHjDLFhQECgIFAQKjhrBgoJAQaAgUBAouIoFCwoCBYGC\n",
       "QEGg4NdfgjpBoCBQECjM9VRjZUFBoCBQECgIFNwwCxYUBAoCBYGCQMFVLFhQECgIFAQKPqSDBQWB\n",
       "gkBBoCBQ8BtmwYKCQEGgIFBw1AgWFAQKAgWBgkDBVSxYUBAoCBQECgIFX2YJ6gSBgkBBoODLLMGC\n",
       "gkBBoCBQECi4YRYsKAgUBAoCBYGCq1iwoCBQECgIFLwGHCwoCBQECgIFgYKjRrCgIFAQKAgU5h0f\n",
       "0hsLCgIFgYJAQaDghlmwoCBQECgIFAQKbpgFCwoCBYGCQMFRI1hQECgIFAQKAgVHjWBBQaAgUBAo\n",
       "CBScxYIFBYGCQEGg4KgRLCgIFAQKAgWBgqtYsKAgUBAoCBTm+jLLyoKCQEGgIFAQKDhqBAsKAgWB\n",
       "gkBBoODZfLCgIFAQKAgU5r1v/wm/zYKCQEGgIFAQKDhqBAsKAgWBgkDBD00GCwoCBYGCQEGg4KgR\n",
       "LCgIFAQKAgWBgjfMggUFgYJAQaDgyyzBgoJAQaAgUBAoeMMsWFAQKAgUBAoCBY99ggUFgYJAQaDg\n",
       "qUawoCBQECgIFAQKHvsECwoCBYGCQMH9oGBBQaAgUBAoCBQ8mw8WFAQKAgWBgkDBDbNgQUGgIFAQ\n",
       "KHg2HywoCBQECgIFgcLcb/8FP86CgkBBoCBQECg4iwULCgIFgYJAwRtmwYKCQEGgIFAQKLhhFiwo\n",
       "CBQECgIF94OCBQWBgkBBoCBQcNQIFhQECgIFgYJAwVksWFAQKAgUBAqezQcLCgIFgYJAQaAwfvxl\n",
       "Z0FBoCBQECgIFJzFggUFgYJAQaDg2XywoCBQECgIFAQKns0HCwoCBYGCQMFRI1hQECgIFAQKAgVH\n",
       "jWBBQaAgUBAoCBTmesVsZUFBoCBQECi4YRYsKAgUBAoCBYGCG2bBgoJAQaAgUHDUCBYUBAoCBYGC\n",
       "QGGepxorCwoCBYGCQEGgMPe4YbaxoCBQECgIFLxAFSwoCBQECgIFgYIfmgwWFAQKAgWBgkDBD00G\n",
       "CwoCBYGCQMGz+WBBQaAgUBAoCBS8Jx0sKAgUBAoCBd84DBYUBAoCBYGCQMFRI1hQECgIFAQKAgXv\n",
       "SQcLCgIFgYJAwWvAwYKCQEGgIFAQKDhqBAsKAgWBgkBBoOA96WBBQaAgUBAo+ImuYEFBoCBQECgI\n",
       "FNwwCxYUBAoCBYGCZ/PBgoJAQaAgUBAoOGoECwoCBYGCQEGg4CwWLCgIFAQKAgVHjWBBQaAgUBAo\n",
       "CBRcxYIFBYGCQEGgIFBwwyxYUBAoCBQECo4awYKCQEGgIFAQKDhqBAsKAgWBgkDBUSNYUBAoCBQE\n",
       "CgIFP9EVLCgIFAQKAgWBgv8vFiwoCBQECgIFN8yCBQWBgkBBoCBQmHdcxjYWFAQKAgWBgkDBWSxY\n",
       "UBAoCBQECp7NBwsKAgWBgkBBoODZfLCgIFAQKAgU3A8KFhQECgIFgYJAwbP5YEFBoCBQECgIFJzF\n",
       "ggUFgYJAQaDgJ7qCBQWBgkBBoCBQmOsVs5UFBYGCQEGg4AWqYEFBoCBQECgIFOY5aqwsKAgUBAoC\n",
       "BYGCs1iwoCBQECgIFDzVCBYUBAoCBYGCQMEbZsGCgkBBoCBQECjM9ZXMlQUFgYJAQaDghlmwoCBQ\n",
       "ECgIFAQKfv0lWFAQKAgUBApeoAoWFAQKAgWBgkDBU41gQUGgIFAQKAgU3DALFhQECgIFgcLc45bZ\n",
       "xoKCQEGgIFAQKLhhFiwoCBQECgIFgYKrWLCgIFAQKAgUPNUIFhQECgIFgYJAwWOfYEFBoCBQECjM\n",
       "/ThqbCwoCBQECgIFgYKjRrCgIFAQKAgUBArzXMVWFhQECgIFgYKjRrCgIFAQKAgUBApzP65iGwsK\n",
       "AgWBgkBBoOAsFiwoCBQECgIFTzWCBQWBgkBBoCBQcNQIFhQECgIFgYIP6WBBQaAgUBAoCBQ8mw8W\n",
       "FAQKAgWBgkBh3vnz7b/hp1lQECgIFAQKbpgFCwoCBYGCQEGg4A2zYEFBoCBQECjMfe4HbSwoCBQE\n",
       "CgIFgYKjRrCgIFAQKAgUBApzPZtfWVAQKAgUBAqOGsGCgkBBoCBQECg4agQLCgIFgYJAQaAw7zmL\n",
       "bSwoCBQECgIFN8yCBQWBgkBBoCBQ8J50sKAgUBAoCBQcNYIFBYGCQEGgIFCY56ixsqAgUBAoCBQE\n",
       "Cs5iwYKCQEGgIFCY6wWqlQUFgYJAQaAgUPCb9sGCgkBBoCBQECj4MkuwoCBQECgIFDzVCBYUBAoC\n",
       "BYGCQMFRI1hQECgIFAQKPqSDBQWBgkBBoCBQ8Gw+WFAQKAgUBAoCBWexYEFBoCBQECjM8Wx+ZUFB\n",
       "oCBQECgIFBw1ggUFgYJAQaAgUPCedLCgIFAQKAgU5jhqrCwoCBQECgIFgYI3zIIFBYGCQEGg4Nl8\n",
       "sKAgUBAoCBQECm6YBQsKAgWBgkBBoDDvvG//DT/NgoJAQaAgUHDDLFhQECgIFAQKAoU5jhorCwoC\n",
       "BYGCQOE/LrLuTX/wFc8AAAAASUVORK5CYII=\n",
       "\" transform=\"translate(1880, 47)\"/>\n",
       "</g>\n",
       "<path clip-path=\"url(#clip740)\" d=\"M 0 0 M1988.07 1483.27 L2017.74 1483.27 L2017.74 1487.21 L1988.07 1487.21 L1988.07 1483.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2032.81 1468.62 Q2029.2 1468.62 2027.37 1472.18 Q2025.57 1475.72 2025.57 1482.85 Q2025.57 1489.96 2027.37 1493.53 Q2029.2 1497.07 2032.81 1497.07 Q2036.45 1497.07 2038.25 1493.53 Q2040.08 1489.96 2040.08 1482.85 Q2040.08 1475.72 2038.25 1472.18 Q2036.45 1468.62 2032.81 1468.62 M2032.81 1464.91 Q2038.62 1464.91 2041.68 1469.52 Q2044.76 1474.1 2044.76 1482.85 Q2044.76 1491.58 2041.68 1496.19 Q2038.62 1500.77 2032.81 1500.77 Q2027 1500.77 2023.92 1496.19 Q2020.87 1491.58 2020.87 1482.85 Q2020.87 1474.1 2023.92 1469.52 Q2027 1464.91 2032.81 1464.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2049.83 1494.22 L2054.71 1494.22 L2054.71 1500.1 L2049.83 1500.1 L2049.83 1494.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2072.63 1469.61 L2060.82 1488.06 L2072.63 1488.06 L2072.63 1469.61 M2071.4 1465.54 L2077.28 1465.54 L2077.28 1488.06 L2082.21 1488.06 L2082.21 1491.95 L2077.28 1491.95 L2077.28 1500.1 L2072.63 1500.1 L2072.63 1491.95 L2057.03 1491.95 L2057.03 1487.44 L2071.4 1465.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1988.07 1243.4 L2017.74 1243.4 L2017.74 1247.34 L1988.07 1247.34 L1988.07 1243.4 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2032.81 1228.75 Q2029.2 1228.75 2027.37 1232.32 Q2025.57 1235.86 2025.57 1242.99 Q2025.57 1250.09 2027.37 1253.66 Q2029.2 1257.2 2032.81 1257.2 Q2036.45 1257.2 2038.25 1253.66 Q2040.08 1250.09 2040.08 1242.99 Q2040.08 1235.86 2038.25 1232.32 Q2036.45 1228.75 2032.81 1228.75 M2032.81 1225.05 Q2038.62 1225.05 2041.68 1229.65 Q2044.76 1234.24 2044.76 1242.99 Q2044.76 1251.71 2041.68 1256.32 Q2038.62 1260.9 2032.81 1260.9 Q2027 1260.9 2023.92 1256.32 Q2020.87 1251.71 2020.87 1242.99 Q2020.87 1234.24 2023.92 1229.65 Q2027 1225.05 2032.81 1225.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2049.83 1254.35 L2054.71 1254.35 L2054.71 1260.23 L2049.83 1260.23 L2049.83 1254.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2073.95 1241.6 Q2077.3 1242.32 2079.18 1244.58 Q2081.08 1246.85 2081.08 1250.19 Q2081.08 1255.3 2077.56 1258.1 Q2074.04 1260.9 2067.56 1260.9 Q2065.38 1260.9 2063.07 1260.46 Q2060.78 1260.05 2058.32 1259.19 L2058.32 1254.68 Q2060.27 1255.81 2062.58 1256.39 Q2064.9 1256.97 2067.42 1256.97 Q2071.82 1256.97 2074.11 1255.23 Q2076.42 1253.5 2076.42 1250.19 Q2076.42 1247.13 2074.27 1245.42 Q2072.14 1243.68 2068.32 1243.68 L2064.29 1243.68 L2064.29 1239.84 L2068.51 1239.84 Q2071.96 1239.84 2073.79 1238.47 Q2075.61 1237.08 2075.61 1234.49 Q2075.61 1231.83 2073.72 1230.42 Q2071.84 1228.98 2068.32 1228.98 Q2066.4 1228.98 2064.2 1229.4 Q2062 1229.82 2059.36 1230.69 L2059.36 1226.53 Q2062.03 1225.79 2064.34 1225.42 Q2066.68 1225.05 2068.74 1225.05 Q2074.06 1225.05 2077.17 1227.48 Q2080.27 1229.88 2080.27 1234.01 Q2080.27 1236.88 2078.62 1238.87 Q2076.98 1240.83 2073.95 1241.6 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1988.07 1003.54 L2017.74 1003.54 L2017.74 1007.47 L1988.07 1007.47 L1988.07 1003.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2032.81 988.883 Q2029.2 988.883 2027.37 992.448 Q2025.57 995.99 2025.57 1003.12 Q2025.57 1010.23 2027.37 1013.79 Q2029.2 1017.33 2032.81 1017.33 Q2036.45 1017.33 2038.25 1013.79 Q2040.08 1010.23 2040.08 1003.12 Q2040.08 995.99 2038.25 992.448 Q2036.45 988.883 2032.81 988.883 M2032.81 985.179 Q2038.62 985.179 2041.68 989.786 Q2044.76 994.369 2044.76 1003.12 Q2044.76 1011.85 2041.68 1016.45 Q2038.62 1021.04 2032.81 1021.04 Q2027 1021.04 2023.92 1016.45 Q2020.87 1011.85 2020.87 1003.12 Q2020.87 994.369 2023.92 989.786 Q2027 985.179 2032.81 985.179 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2049.83 1014.48 L2054.71 1014.48 L2054.71 1020.36 L2049.83 1020.36 L2049.83 1014.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2063.81 1016.43 L2080.13 1016.43 L2080.13 1020.36 L2058.18 1020.36 L2058.18 1016.43 Q2060.85 1013.67 2065.43 1009.05 Q2070.04 1004.39 2071.22 1003.05 Q2073.46 1000.53 2074.34 998.791 Q2075.24 997.031 2075.24 995.341 Q2075.24 992.587 2073.3 990.851 Q2071.38 989.115 2068.28 989.115 Q2066.08 989.115 2063.62 989.879 Q2061.19 990.642 2058.42 992.193 L2058.42 987.471 Q2061.24 986.337 2063.69 985.758 Q2066.15 985.179 2068.18 985.179 Q2073.55 985.179 2076.75 987.865 Q2079.94 990.55 2079.94 995.041 Q2079.94 997.17 2079.13 999.091 Q2078.35 1000.99 2076.24 1003.58 Q2075.66 1004.25 2072.56 1007.47 Q2069.46 1010.67 2063.81 1016.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M1988.07 763.669 L2017.74 763.669 L2017.74 767.604 L1988.07 767.604 L1988.07 763.669 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2032.81 749.016 Q2029.2 749.016 2027.37 752.581 Q2025.57 756.122 2025.57 763.252 Q2025.57 770.358 2027.37 773.923 Q2029.2 777.465 2032.81 777.465 Q2036.45 777.465 2038.25 773.923 Q2040.08 770.358 2040.08 763.252 Q2040.08 756.122 2038.25 752.581 Q2036.45 749.016 2032.81 749.016 M2032.81 745.312 Q2038.62 745.312 2041.68 749.919 Q2044.76 754.502 2044.76 763.252 Q2044.76 771.979 2041.68 776.585 Q2038.62 781.168 2032.81 781.168 Q2027 781.168 2023.92 776.585 Q2020.87 771.979 2020.87 763.252 Q2020.87 754.502 2023.92 749.919 Q2027 745.312 2032.81 745.312 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2049.83 774.618 L2054.71 774.618 L2054.71 780.497 L2049.83 780.497 L2049.83 774.618 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2060.59 776.562 L2068.23 776.562 L2068.23 750.196 L2059.92 751.863 L2059.92 747.604 L2068.18 745.937 L2072.86 745.937 L2072.86 776.562 L2080.5 776.562 L2080.5 780.497 L2060.59 780.497 L2060.59 776.562 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2000.01 509.149 Q1996.4 509.149 1994.57 512.713 Q1992.77 516.255 1992.77 523.385 Q1992.77 530.491 1994.57 534.056 Q1996.4 537.597 2000.01 537.597 Q2003.65 537.597 2005.45 534.056 Q2007.28 530.491 2007.28 523.385 Q2007.28 516.255 2005.45 512.713 Q2003.65 509.149 2000.01 509.149 M2000.01 505.445 Q2005.82 505.445 2008.88 510.051 Q2011.96 514.635 2011.96 523.385 Q2011.96 532.111 2008.88 536.718 Q2005.82 541.301 2000.01 541.301 Q1994.2 541.301 1991.12 536.718 Q1988.07 532.111 1988.07 523.385 Q1988.07 514.635 1991.12 510.051 Q1994.2 505.445 2000.01 505.445 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2000.01 269.281 Q1996.4 269.281 1994.57 272.846 Q1992.77 276.388 1992.77 283.517 Q1992.77 290.624 1994.57 294.189 Q1996.4 297.73 2000.01 297.73 Q2003.65 297.73 2005.45 294.189 Q2007.28 290.624 2007.28 283.517 Q2007.28 276.388 2005.45 272.846 Q2003.65 269.281 2000.01 269.281 M2000.01 265.578 Q2005.82 265.578 2008.88 270.184 Q2011.96 274.767 2011.96 283.517 Q2011.96 292.244 2008.88 296.851 Q2005.82 301.434 2000.01 301.434 Q1994.2 301.434 1991.12 296.851 Q1988.07 292.244 1988.07 283.517 Q1988.07 274.767 1991.12 270.184 Q1994.2 265.578 2000.01 265.578 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2017.03 294.883 L2021.91 294.883 L2021.91 300.763 L2017.03 300.763 L2017.03 294.883 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2027.79 296.827 L2035.43 296.827 L2035.43 270.462 L2027.12 272.128 L2027.12 267.869 L2035.38 266.203 L2040.06 266.203 L2040.06 296.827 L2047.7 296.827 L2047.7 300.763 L2027.79 300.763 L2027.79 296.827 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2000.01 29.414 Q1996.4 29.414 1994.57 32.9788 Q1992.77 36.5204 1992.77 43.65 Q1992.77 50.7565 1994.57 54.3213 Q1996.4 57.8629 2000.01 57.8629 Q2003.65 57.8629 2005.45 54.3213 Q2007.28 50.7565 2007.28 43.65 Q2007.28 36.5204 2005.45 32.9788 Q2003.65 29.414 2000.01 29.414 M2000.01 25.7103 Q2005.82 25.7103 2008.88 30.3168 Q2011.96 34.9001 2011.96 43.65 Q2011.96 52.3768 2008.88 56.9833 Q2005.82 61.5666 2000.01 61.5666 Q1994.2 61.5666 1991.12 56.9833 Q1988.07 52.3768 1988.07 43.65 Q1988.07 34.9001 1991.12 30.3168 Q1994.2 25.7103 2000.01 25.7103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2017.03 55.0157 L2021.91 55.0157 L2021.91 60.8953 L2017.03 60.8953 L2017.03 55.0157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M 0 0 M2031.01 56.9601 L2047.33 56.9601 L2047.33 60.8953 L2025.38 60.8953 L2025.38 56.9601 Q2028.05 54.2055 2032.63 49.5759 Q2037.23 44.9232 2038.42 43.5806 Q2040.66 41.0574 2041.54 39.3213 Q2042.44 37.5621 2042.44 35.8723 Q2042.44 33.1177 2040.5 31.3816 Q2038.58 29.6455 2035.48 29.6455 Q2033.28 29.6455 2030.82 30.4093 Q2028.39 31.1732 2025.61 32.7241 L2025.61 28.002 Q2028.44 26.8677 2030.89 26.289 Q2033.35 25.7103 2035.38 25.7103 Q2040.75 25.7103 2043.95 28.3955 Q2047.14 31.0806 2047.14 35.5714 Q2047.14 37.701 2046.33 39.6223 Q2045.54 41.5204 2043.44 44.113 Q2042.86 44.7843 2039.76 48.0018 Q2036.66 51.1963 2031.01 56.9601 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1952.07,1486.45 1952.07,1486.45 1976.07,1486.45 1952.07,1486.45 1952.07,1246.58 1976.07,1246.58 1952.07,1246.58 1952.07,1006.71 1976.07,1006.71 1952.07,1006.71 \n",
       "  1952.07,766.846 1976.07,766.846 1952.07,766.846 1952.07,526.979 1976.07,526.979 1952.07,526.979 1952.07,287.111 1976.07,287.111 1952.07,287.111 1952.07,47.2441 \n",
       "  1976.07,47.2441 1952.07,47.2441 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "spy(L, markersize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a simple dynamic valuation, consider if the payoff of being in state $ (i,j) $ is $ r_{ij} = i + 2j $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Float64,1}:\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       " 6.0\n",
       " 5.0\n",
       " 6.0\n",
       " 7.0\n",
       " 8.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [i + 2.0j for i in 1:N, j in 1:M]\n",
    "r = vec(r)  # vectorize it since stacked in same order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the equation $ \\rho v = r + L v $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float64,2}:\n",
       "  87.8992   93.6134\n",
       "  96.1345  101.849\n",
       " 106.723   112.437\n",
       " 114.958   120.672"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ρ = 0.05\n",
    "v = (ρ * I - L) \\ r\n",
    "reshape(v, N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reshape` helps to rearrange it back to being two-dimensional.\n",
    "\n",
    "To find the stationary distribution, we calculate the eigenvalue and choose the eigenvector associated with $ \\lambda=0 $ .  In this\n",
    "case, we can verify it is the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Float64,1}:\n",
       " 0.1666666666666664\n",
       " 0.16666666666666632\n",
       " 0.16666666666666669\n",
       " 0.166666666666667\n",
       " 0.08333333333333312\n",
       " 0.08333333333333345\n",
       " 0.08333333333333337\n",
       " 0.0833333333333335"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_eig = eigen(Matrix(L'))\n",
    "@assert norm(L_eig.values[end]) < 1E-10\n",
    "\n",
    "ψ = L_eig.vectors[:,end]\n",
    "ψ = ψ / sum(ψ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping this to be two dimensional if it is helpful for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float64,2}:\n",
       " 0.166667  0.0833333\n",
       " 0.166667  0.0833333\n",
       " 0.166667  0.0833333\n",
       " 0.166667  0.0833333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(ψ, N, size(A,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irreducibility\n",
    "\n",
    "As with the discrete time Markov chains, a key question is whether CTMCs are reducible, i.e. states communicate.  The problem\n",
    "is isomorphic to determining if the directed graph of the Markov chain is [strongly connected](https://en.wikipedia.org/wiki/Strongly_connected_component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " -0.1   0.1    ⋅     ⋅     ⋅     ⋅ \n",
       "  0.1  -0.2   0.1    ⋅     ⋅     ⋅ \n",
       "   ⋅    0.1  -0.2   0.1    ⋅     ⋅ \n",
       "   ⋅     ⋅    0.1  -0.2   0.1    ⋅ \n",
       "   ⋅     ⋅     ⋅    0.1  -0.2   0.1\n",
       "   ⋅     ⋅     ⋅     ⋅    0.1  -0.1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LightGraphs\n",
    "α = 0.1\n",
    "N = 6\n",
    "Q = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that it is possible to move between every state in a finite number of steps with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_strongly_connected(Q_graph) = true\n"
     ]
    }
   ],
   "source": [
    "Q_graph = DiGraph(Q)\n",
    "@show is_strongly_connected(Q_graph);  # i.e. can follow directional edges to get to every state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, as an example of a reducible Markov chain where states $ 1 $ and $ 2 $ cannot jump to state $ 3 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_strongly_connected(Q_graph) = false\n"
     ]
    }
   ],
   "source": [
    "Q = [-0.2 0.2 0\n",
    "    0.2 -0.2 0\n",
    "    0.2 0.6 -0.8]\n",
    "Q_graph = DiGraph(Q)\n",
    "@show is_strongly_connected(Q_graph);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banded Matrices\n",
    "\n",
    "A tridiagonal matrix has 3 non-zero diagonals.  The main diagonal, the first sub-diagonal (i.e. below the main diagonal) and also the first super-diagonal (i.e. above the main diagonal).\n",
    "\n",
    "This is a special case of a more general type called a banded matrix, where the number of sub- and super-diagonals can be greater than 1.  The\n",
    "total width of main-, sub-, and super-diagonals is called the bandwidth.  For example, a tridiagonal matrix has a bandwidth of 3.\n",
    "\n",
    "A $ N \\times N $ banded matrix with bandwidth $ P $ has about $ N P $ nonzeros in its sparsity pattern.\n",
    "\n",
    "These can be created directly as a dense matrix with `diagm`.  For example, with a bandwidth of three and a zero diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 0  1  0  0\n",
       " 4  0  2  0\n",
       " 0  5  0  3\n",
       " 0  0  6  0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagm(1 => [1,2,3], -1 => [4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a sparse matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 SparseMatrixCSC{Int64,Int64} with 6 stored entries:\n",
       "  [2, 1]  =  4\n",
       "  [1, 2]  =  1\n",
       "  [3, 2]  =  5\n",
       "  [2, 3]  =  2\n",
       "  [4, 3]  =  6\n",
       "  [3, 4]  =  3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdiagm(1 => [1,2,3], -1 => [4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, directly using [BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 BandedMatrix{Int64,Array{Int64,2},Base.OneTo{Int64}}:\n",
       " 0  1  ⋅  ⋅\n",
       " 4  0  2  ⋅\n",
       " ⋅  5  0  3\n",
       " ⋅  ⋅  6  0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BandedMatrices\n",
    "BandedMatrix(1 => [1,2,3], -1 => [4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a convenience function for generating random banded matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 BandedMatrix{Float64,Array{Float64,2},Base.OneTo{Int64}}:\n",
       " 0.386608  0.346262    ⋅         ⋅          ⋅          ⋅         ⋅ \n",
       " 0.479616  0.750276   0.50362    ⋅          ⋅          ⋅         ⋅ \n",
       " 0.857743  0.913585   0.087322  0.0676183   ⋅          ⋅         ⋅ \n",
       " 0.779364  0.293782   0.269804  0.813762   0.147221    ⋅         ⋅ \n",
       "  ⋅        0.0341229  0.711412  0.438157   0.0312296  0.930633   ⋅ \n",
       "  ⋅         ⋅         0.412892  0.351496   0.701733   0.335451  0.0827553\n",
       "  ⋅         ⋅          ⋅        0.394056   0.460506   0.25927   0.418861"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = brand(7, 7, 3, 1)  # 7x7 matrix, 3 subdiagonals, 1 superdiagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, specialized algorithms will be used to exploit the structure when solving linear systems.  In particular, the complexity is related to the $ O(N P_L P_U) $ for upper and lower bandwidths $ P $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factorize(Symmetric(A)) |> typeof = LDLt{Float64,Symmetric{Float64,BandedMatrix{Float64,Array{Float64,2},Base.OneTo{Int64}}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       " -0.6345917189136552\n",
       "  1.2689275835805585\n",
       "  0.549940472179349\n",
       "  0.24947160343942432\n",
       " -0.45227412611006496\n",
       "  0.49732000255918096\n",
       "  1.3752489574369147"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show factorize(Symmetric(A)) |> typeof\n",
    "A \\ rand(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factorization algorithm uses a specialized LU decomposition for banded matrices.\n",
    "\n",
    "\n",
    "<a id='implementation-numerics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Performance\n",
    "\n",
    "Recall the famous quote from Knuth: “97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%”.  The most common example of premature optimization is trying to use your own mental model of a compiler while writing your code, worried about the efficiency of code and (usually incorrectly) second-guessing the compiler.\n",
    "\n",
    "Concretely, the lessons in this section are\n",
    "\n",
    "1. Don’t worry about optimizing your code unless you need to.  Code clarity is your first-order concern.  \n",
    "1. If you use other people’s packages, they can worry about performance and you don’t need to.  \n",
    "1. If you absolutely need that “critical 3%” your intuition about performance is usually wrong on modern CPUs and GPUs, so let the compiler do its job.  \n",
    "1. Benchmarking (e.g. `@btime`) and [profiling](https://docs.julialang.org/en/v1/manual/profile/) are the tools to figure out performance bottlenecks.  If 99% of computing time is spent in 1 small function, then there is no point optimizing anything else.  \n",
    "1. If you benchmark to show that a particular part of the code is an issue, and you can’t find another library that does a better job, then you can worry about performance.  \n",
    "\n",
    "\n",
    "You will rarely get to step 3, let alone step 5.\n",
    "\n",
    "However, there is also a corollary:  “don’t pessimize prematurely”. That is, don’t make choices that lead to poor performance without any tradeoff in improved code clarity.  For example, writing your own algorithms when a high performance algorithm exists in a package or Julia itself, or lazily making a matrix dense and carelessly dropping its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Difficulty\n",
    "\n",
    "Numerical analysts sometimes refer to the lowest level of code for basic operations (e.g. a dot product, matrix-matrix product, convolutions) as `kernels`.\n",
    "\n",
    "That sort of code is difficult to write, and performance depends on the characteristics of the underlying hardware such as the [instruction set](https://en.wikipedia.org/wiki/Instruction_set_architecture) available on the particular CPU, the size of the [CPU cache](https://en.wikipedia.org/wiki/CPU_cache), and the layout of arrays in memory.\n",
    "\n",
    "Typically these operations are written in a [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) library, organized into different levels.  The levels roughly correspond to the computational order of the operations:  BLAS Level 1 are $ O(N) $ operations such as linear products, Level 2 are $ O(N^2) $ operations such as matrix-vector products, and Level 3 are roughly $ O(N^3) $ such as general matrix-matrix products.\n",
    "\n",
    "An example of a BLAS library is [OpenBLAS](https://github.com/xianyi/OpenBLAS) used by default in Julia, or  the [Intel MKL](https://en.wikipedia.org/wiki/Math_Kernel_Library) used in Matlab (and Julia if the `MKL.jl` package is installed).\n",
    "\n",
    "On top of BLAS are [LAPACK](https://en.wikipedia.org/wiki/LAPACK) operations, which are higher level kernels, such as matrix factorizations and eigenvalue algorithms, and are often in the same libraries (e.g. MKL has both BLAS and LAPACK functionality).\n",
    "\n",
    "The details of these packages are not especially relevant, but if you are talking about performance, people will inevitably start discussing these different packages and kernels.  There are a few important things to keep in mind:\n",
    "\n",
    "1. Leave writing kernels to the experts.  Even simple sounding algorithms can be very complicated to implement with high performance.  \n",
    "1. Your intuition about performance of code is probably going to be wrong.  If you use high quality libraries rather than writing your own kernels, you don’t need to use your intuition.  \n",
    "1. Don’t get distracted by the jargon or acronyms above if you are reading about performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row and Column-Major Ordering\n",
    "\n",
    "There is a practical performance issue which may influence your code.  Since memory in a CPU is linear, dense matrices need to be stored by either stacking columns (called [column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order)) or rows.\n",
    "\n",
    "The reason this matters is that compilers can generate better performance if they work in contiguous chunks of memory, and this becomes especially important with large matrices due to the interaction with the CPU cache.  Choosing the wrong order when there is no benefit in code clarity is a an example of premature pessimization.  The performance difference can be orders of magnitude in some cases, and nothing in others.\n",
    "\n",
    "One option is to use the functions that let the compiler choose the most efficient way to traverse memory. If you need to choose the looping order yourself, then you might want to experiment with swapping whether you go through columns or rows first.  Other times, let Julia decide, i.e. `enumerate` and `eachindex` will choose the right approach.\n",
    "\n",
    "Julia, Fortran, and Matlab all use column-major order while C/C++ and Python use row-major order.  This means that if you find an algorithm written for C/C++/Python you will sometimes need to make small changes if performance is an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression on Allocations and Inplace Operations\n",
    "\n",
    "While we have usually not considered optimizing code for performance (and focused on the choice of\n",
    "algorithms instead), when matrices and vectors become large we need to be more careful.\n",
    "\n",
    "The most important thing to avoid are excess allocations, which usually occur due to the use of\n",
    "temporary vectors and matrices when they are not necessary.  Sometimes those extra temporary values\n",
    "can cause enormous degredations in performance.\n",
    "\n",
    "However, caution is suggested since\n",
    "excess allocations are never relevant for scalar values, and can sometimes create faster code for\n",
    "smaller matrices/vectors since it can lead to better [cache locality](https://en.wikipedia.org/wiki/Locality_of_reference).\n",
    "\n",
    "To see this, a convenient tool is the benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  447.162 ns (1 allocation: 896 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float64,2}:\n",
       " 3.26172  4.24569  3.37182  4.10324  …  4.03344  4.39198  2.88797  2.63934\n",
       " 4.19687  4.58126  3.88015  5.0409      4.0105   3.56832  2.35475  3.32362\n",
       " 2.17535  2.58069  3.08736  3.04461     2.71563  3.03535  2.62734  2.37854\n",
       " 4.07043  4.57067  4.23989  5.24296     4.34443  4.21237  3.30526  3.82245\n",
       " 3.16928  4.20751  3.08482  3.89843     3.81516  4.14681  2.64178  2.94961\n",
       " 3.01031  3.08903  2.83417  3.80852  …  3.22832  3.29357  2.57282  2.60746\n",
       " 3.88276  4.45627  3.88941  5.12798     4.11822  3.70176  2.69528  3.81814\n",
       " 2.7023   3.10147  2.95828  3.63363     3.64397  3.40609  2.44341  3.03272\n",
       " 3.02687  3.13864  2.78748  3.90634     3.18422  2.90128  1.99457  2.80653\n",
       " 3.80929  3.83031  3.88255  4.8596      4.16155  3.73634  2.65279  3.07034"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "A = rand(10,10)\n",
    "B = rand(10,10)\n",
    "C = similar(A)\n",
    "function f!(C, A, B)\n",
    "    D = A*B\n",
    "    C .= D .+ 1\n",
    "end\n",
    "@btime f!($C, $A, $B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `!` on the `f!` is an informal way to say that the function is mutating, and the first arguments (`C` here)\n",
    "is by convention the modified values.\n",
    "\n",
    "In the `f!` function, notice that the `D` is a temporary variable which is created, and then modified afterwards.  But, notice that since\n",
    "`C` is modified directly, there is no need to create the temporary `D` matrix.\n",
    "\n",
    "This is an example of where an inplace version of the matrix multiplication can help avoid the allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  444.404 ns (1 allocation: 896 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  373.946 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float64,2}:\n",
       " 2.42733  3.74571  2.5811   2.745    …  2.45258  3.17339  2.792    3.46213\n",
       " 3.52188  4.16932  3.17155  3.98401     2.1202   2.85629  3.35848  3.88871\n",
       " 3.74317  4.66988  3.3338   4.69372     2.61622  3.70894  4.06268  4.79582\n",
       " 3.30158  4.09369  3.81428  3.65591     2.743    3.42494  3.65687  3.83879\n",
       " 2.47181  4.33343  2.46863  2.68593     2.38238  3.6709   3.2434   4.17783\n",
       " 3.5594   4.72281  3.71072  4.31957  …  2.83065  4.21896  4.34601  4.90251\n",
       " 3.76742  4.85555  4.03515  4.55265     2.62424  4.19292  4.57003  4.88181\n",
       " 3.29688  5.38813  3.4278   3.8622      2.87482  4.07336  3.89498  5.41919\n",
       " 2.96602  3.60521  2.90236  3.2117      2.68528  2.99728  3.34362  3.47657\n",
       " 4.73208  5.38525  4.42378  5.18235     2.91664  4.70184  5.28638  5.4401"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f2!(C, A, B)\n",
    "    mul!(C, A, B)  # in place multiplication\n",
    "    C .+= 1\n",
    "end\n",
    "A = rand(10,10)\n",
    "B = rand(10,10)\n",
    "C = similar(A)\n",
    "@btime f!($C, $A, $B)\n",
    "@btime f2!($C, $A, $B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the output of the benchmarking, the `f2!` is non-allocating and is using the pre-allocated `C` variable directly.\n",
    "\n",
    "Another example of this is solutions to linear equations, where for large solutions you may pre-callocate and reuse the\n",
    "solution vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " -0.09745394360765199\n",
       "  0.7799354221131607\n",
       "  1.1994346228906076\n",
       "  0.0913844576787106\n",
       " -0.5083914639425641\n",
       " -0.35091623556086166\n",
       "  0.793473061987608\n",
       " -0.5304171009174151\n",
       "  0.4517444530913054\n",
       " -0.8005334538688567"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(10,10)\n",
    "y = rand(10)\n",
    "z = A \\ y  # creates temporary\n",
    "\n",
    "A = factorize(A)  # inplace requires factorization\n",
    "x = similar(y)  # pre-allocate\n",
    "ldiv!(x, A, y)  # inplace left divide, using factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you benchmark carefully, you will see that this is sometimes slower.  Avoiding allocations is not always a good\n",
    "idea - and worrying about it prior to benchmarking is premature optimization.\n",
    "\n",
    "There are a variety of other non-allocating versions of functions.  For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float64,2}:\n",
       " 0.373481   0.715094  0.880197   0.219559   …  0.903144   0.0534784  0.646242\n",
       " 0.0572854  0.437244  0.0465054  0.271735      0.0419775  0.91462    0.804396\n",
       " 0.0722476  0.435665  0.631825   0.0804549     0.773098   0.254097   0.674881\n",
       " 0.0341739  0.185395  0.736277   0.142816      0.687287   0.236726   0.19037\n",
       " 0.843743   0.860459  0.709686   0.630887      0.274137   0.958363   0.948974\n",
       " 0.918731   0.933097  0.280531   0.486534   …  0.0313851  0.479192   0.988241\n",
       " 0.868133   0.243504  0.628518   0.954309      0.667845   0.935099   0.990551\n",
       " 0.0636638  0.659151  0.377286   0.0453235     0.865368   0.64157    0.570134\n",
       " 0.759633   0.389194  0.153783   0.284574      0.245533   0.516012   0.55121\n",
       " 0.301123   0.505073  0.0402959  0.225074      0.57159    0.893165   0.374389"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(10,10)\n",
    "B = similar(A)\n",
    "\n",
    "transpose!(B, A)  # non-allocating version of B = transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a common source of unnecessary allocations is when taking slices or portions of\n",
    "matrices.  For example, the following allocates a new matrix `B` and copies the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.07265755245781103\n",
       " 0.2967203620355736\n",
       " 0.7745398448673058\n",
       " 0.6244448536072318\n",
       " 0.5287113274542306"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(5,5)\n",
    "B = A[2,:]  # extract a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see these are different matrices, note that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A[2, 1] = 100.0\n",
      "B[1] = 0.07265755245781103\n"
     ]
    }
   ],
   "source": [
    "A[2,1] = 100.0\n",
    "@show A[2,1]\n",
    "@show B[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of allocating a new matrix, you can take a `view` of a matrix, which provides an\n",
    "appropriate `AbstractArray` type that doesn’t allocate new memory with the `@view` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A[2, 1] = 100.0\n",
      "B[1] = 100.0\n"
     ]
    }
   ],
   "source": [
    "A = rand(5,5)\n",
    "B = @view A[2,:]  #  does not copy the data\n",
    "\n",
    "A[2,1] = 100.0\n",
    "@show A[2,1]\n",
    "@show B[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, again, you will often find that doing `@view` leads to slower code.  Benchmark\n",
    "instead, and generally rely on it for large matrices and for contiguous chunks of memory (e.g. a column rather than a row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "This exercise is for a practice on writing low-level routines (i.e. “kernels”), and to hopefully convince you to leave low-level code to the experts.\n",
    "\n",
    "The formula for matrix multiplication is deceptively simple.  For example, with the product of square matrices $ C = A B $ of size $ N \\times N $, the $ i,j $ element of $ C $ is\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^N A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "Alternatively, you can take a row $ A_{i,:} $ and column $ B_{:, j} $ and use an inner product\n",
    "\n",
    "$$\n",
    "C_{ij} = A_{i,:} \\cdot B_{:,j}\n",
    "$$\n",
    "\n",
    "Note that the inner product in a discrete space is simply a sum, and has the same complexity as the sum (i.e. $ O(N) $ operations).\n",
    "\n",
    "For a dense matrix without any structure using a naive multiplication algorithm, this also makes it clear why the complexity is $ O(N^3) $: you need to evaluate it for $ N^2 $ elements in the matrix and do an $ O(N) $ operation each time.\n",
    "\n",
    "For this exercise, implement matrix multiplication yourself and compare performance in a few permutations.\n",
    "\n",
    "1. Use the built-in function in Julia (i.e.``C = A * B`` or, for a better comparison, the inplace version `mul!(C, A, B)` which works with preallocated data)  \n",
    "1. Loop over each $ C_{ij} $ by the row first (i.e. the `i` index) and use a `for` loop for the inner product  \n",
    "1. Loop over each $ C_{ij} $ by the column first (i.e. the `j` index) and use a `for` loop for the inner product  \n",
    "1. Do the same but use the `dot` product instead of the sum.  \n",
    "1. Choose your best implementation of these, and then for matrices of a few different sizes `N=10`, `N=1000`, etc. and compare the ratio of performance of your best implementation to the built in BLAS library.  \n",
    "\n",
    "\n",
    "A few more hints:\n",
    "\n",
    "- You can just use random matrices, e.g. `A = rand(N, N)`, etc.  \n",
    "- For all of them, preallocate the $ C $ matrix beforehand with `C = similar(A)` or something equivalent.  \n",
    "- To compare performance, put your code in a function and use `@btime` macro to time it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2a\n",
    "\n",
    "Here we will calculate the evolution of the pdf of a discrete time Markov Chain, $ \\psi_t $ given the initial condition $ \\psi_0 $.\n",
    "\n",
    "Start with a simple symmetric tridiagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2)])\n",
    "A_adjoint = A';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pick some large `T` and use the initial condition $ \\psi_0 = \\begin{bmatrix} 1 & 0 & \\ldots & 0\\end{bmatrix} $  \n",
    "1. Write code to calculate $ \\psi_t $ to some $ T $ by iterating the map for each $ t $, i.e.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\psi_{t+1} = A' \\psi_t\n",
    "$$\n",
    "\n",
    "1. What is the computational order of calculating  $ \\psi_T $ using this iteration approach $ T < N $?  \n",
    "1. What is the computational order of $ (A')^T = (A' \\ldots A') $ and then $ \\psi_T = (A')^T \\psi_0 $ for $ T < N $?  \n",
    "1. Benchmark calculating $ \\psi_T $ with the iterative calculation above as well as the direct $ \\psi_T = (A')^T \\psi_0 $ to see which is faster.  You can take the matrix power with just `A_adjoint^T`, which uses specialized algorithms faster and more accurate than repeated matrix multiplication (but with the same computational order).  \n",
    "1. Check the same if $ T = 2 N $  \n",
    "\n",
    "\n",
    "*Note:* The algorithm used in Julia to take matrix powers  depends on the matrix structure, as always.  In the symmetric case, it can use an eigendecomposition, whereas with a general dense matrix it uses [squaring and scaling](https://doi.org/10.1137/090768539)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b\n",
    "\n",
    "With the same setup as Exercise 2a, do an [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of `A_transpose`.  That is, use `eigen` to factorize the adjoint $ A' = Q \\Lambda Q^{-1} $ where $ Q $ the matrix of eigenvectors and $ \\Lambda $ the diagonal matrix of eigenvalues.  Calculate $ Q^{-1} $ from the results.\n",
    "\n",
    "Use the factored matrix to calculate the sequence of $ \\psi_t = (A')^t \\psi_0 $ using the relationship\n",
    "\n",
    "$$\n",
    "\\psi_t = Q \\Lambda^t Q^{-1} \\psi_0\n",
    "$$\n",
    "\n",
    "Where matrix powers of diagonal matrices are simply the element-wise power of each element.\n",
    "\n",
    "Benchmark the speed of calculating the sequence of $ \\psi_t $ up to `T = 2N` using this method.  In principle, the factorization and easy calculation of the power should give you benefits compared to simply iterating the map as we did in Exercise 2a.  Explain why it does or does not using computational order of each approach."
   ]
  }
 ],
 "metadata": {
  "date": 1589930312.243192,
  "download_nb": true,
  "download_nb_path": "https://julia.quantecon.org/",
  "filename": "numerical_linear_algebra.rst",
  "filename_with_path": "tools_and_techniques/numerical_linear_algebra",
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "title": "Numerical Linear Algebra and Factorizations"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
